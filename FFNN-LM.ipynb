{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d_q6vcf9S0Tf"
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils import data\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import string\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hdxpunGIZxa",
    "outputId": "4ff401f1-1ea6-498e-f8a7-b57fafc8b6a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "# Clear cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "# Use GPU if available otherwise use CPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda: print(\"Using GPU\") \n",
    "else: print(\"Using CPU\")\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2ppPDEg-uMn",
    "outputId": "25abf541-f4af-4ddd-c74e-a10245697903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to use SGD or Adam optimizer? Enter 1 for SGD or enter 2 for Adam, then press ENTER2\n"
     ]
    }
   ],
   "source": [
    "# Choose whether to use SGD or Adam optimizer\n",
    "# global optimizer_choice\n",
    "# optimizer_choice = input(\"Do you want to use SGD or Adam optimizer? Enter 1 for SGD or enter 2 for Adam, then press ENTER\")\n",
    "optimizer_choice = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ARcM-s5hT-qA"
   },
   "outputs": [],
   "source": [
    "# Load document into memory\n",
    "def load_document(filename) -> str:\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text in the file\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p4lIvXrea8Sk"
   },
   "outputs": [],
   "source": [
    "# Load train dataset\n",
    "train_filename = '/content/drive/MyDrive/LMDatasets/nchlt_text.zu.train'\n",
    "train_document = load_document(train_filename)\n",
    "# Load validation dataset\n",
    "valid_filename = '/content/drive/MyDrive/LMDatasets/nchlt_text.zu.valid'\n",
    "valid_document = load_document(valid_filename)\n",
    "# Load test dataset\n",
    "test_filename = '/content/drive/MyDrive/LMDatasets/nchlt_text.zu.test'\n",
    "test_document = load_document(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qtJR0YTmTIs2"
   },
   "outputs": [],
   "source": [
    "# Create tokens from the document\n",
    "def clean_data(document) -> list :\n",
    "    # Split by sentences\n",
    "    sentences = document.split(\".\")\n",
    "    # Split sentences into words\n",
    "    words = [x.split() for x in sentences]\n",
    "\n",
    "    # Remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for i,s in enumerate(words):\n",
    "      words[i] = [w.translate(table) for w in s]\n",
    "      # Normalise to lower case\n",
    "      words[i] = [word.lower() for word in s]\n",
    "      # Remove empty list index using list comprehension\n",
    "      words[i] = [i for i in words[i] if i]\n",
    "\n",
    "    return words\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuUFNphae2OX",
    "outputId": "4fab89ee-eb31-4f6e-d41c-fea790191b7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 94039\n",
      "First sentence: ['funda', 'umhlahlandlela', 'wesicelo', 'sokujoyina', 'ekhasini', '7', 'ngokucophelela,', 'njengoba', 'uqukethe', 'ulwazi', 'olubalulekile', 'oluzokusiza', 'ekugcwaliseni', 'ifomu', 'lokufaka', 'isicelo', 'sokujoyina', 'ngendlela', 'efanele']\n"
     ]
    }
   ],
   "source": [
    "# Call function to clean document and view number of tokens\n",
    "train_sentence = clean_data(train_document)\n",
    "\n",
    "print('Total Tokens: %d' % len(train_sentence))\n",
    "print(f\"First sentence: {train_sentence[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Vd4WzXX8XfUF"
   },
   "outputs": [],
   "source": [
    "def unknownWords(words) :\n",
    "    # From word mapping to index, create a dictionary of count of words\n",
    "    words_dict = {}\n",
    "    for index, word in enumerate(words) :\n",
    "      for w in words[index] :\n",
    "        if w in words_dict:\n",
    "            words_dict[w] +=1\n",
    "        else:\n",
    "            words_dict[w] = 1\n",
    "\n",
    "    # Create train set and replace all words that occur only once in the training data with UNK token\n",
    "    for index, word in enumerate(words) :\n",
    "      for i, w in enumerate(words[index]) :\n",
    "        if words_dict[w] == 1 :\n",
    "            words[index][i] = \"UNK\"\n",
    "    \n",
    "    train_set = []\n",
    "    for sentence in words:\n",
    "      for word in sentence:\n",
    "          train_set.append(word)\n",
    "\n",
    "    return words, train_set, words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VQVaUpnNIZxe"
   },
   "outputs": [],
   "source": [
    "train_words, train_set, test_words_dict = unknownWords(train_sentence)\n",
    "# Create training vocabulary\n",
    "train_vocabulary = list(set(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ex3hPAQAY0VZ",
    "outputId": "17d0f4c8-cceb-4eb0-e51d-b209bf99691e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total TrainSet: 1294312\n",
      "Total Vocabulary: 81865\n",
      "TrainSet UNK: 133954\n"
     ]
    }
   ],
   "source": [
    "print('Total TrainSet: %d' % len(train_set))\n",
    "print('Total Vocabulary: %d' % len(set(train_vocabulary)))\n",
    "print(f\"TrainSet UNK: {train_set.count('UNK')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3ewu34llIZxf"
   },
   "outputs": [],
   "source": [
    "# Create word-index mapping vocabulary for constructing context\n",
    "train_word_to_index={train_word: i for i, train_word in enumerate(train_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fvmx-5q-e9LA"
   },
   "outputs": [],
   "source": [
    "# Build the trigrams from the list of training set\n",
    "def buid_trigrams(train_words, word_to_index) :\n",
    "  trigrams=[]\n",
    "  torch_trigrams_input=[]\n",
    "  torch_trigrams_output=[]\n",
    "  \n",
    "  for index, sentence in enumerate(train_words) :\n",
    "    for i in range (len(train_words[index])-2):\n",
    "      context=[]\n",
    "      # Create trigram\n",
    "      trigrams.append(([train_words[index][i],train_words[index][i+1]],train_words[index][i+2]))\n",
    "      # Build context\n",
    "      context.append(word_to_index[train_words[index][i]])\n",
    "      context.append(word_to_index[train_words[index][i+1]])\n",
    "      target = word_to_index[train_words[index][i+2]]\n",
    "      \n",
    "      # Prepare input and output tensors for torch dataset\n",
    "      torch_trigrams_input.append(torch.tensor(context,dtype=torch.long,device=device))\n",
    "      torch_trigrams_output.append(torch.tensor([target],dtype=torch.long,device=device))\n",
    "\n",
    "  return (torch_trigrams_input, torch_trigrams_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZPGpJZjgOZFK"
   },
   "outputs": [],
   "source": [
    "torch_train_trigrams_input, torch_train_trigrams_output = buid_trigrams(train_words, train_word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l38X6ISB5iQm",
    "outputId": "a4f665af-b0b3-4b4a-929b-bd72e7b4ea12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([39089, 34048], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch_train_trigrams_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUNXwKsAlhtm",
    "outputId": "a363ee11-78b9-42fc-ae28-c8319a294671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([39089, 34048], device='cuda:0'), tensor([34048, 76221], device='cuda:0'), tensor([76221,  8030], device='cuda:0')]\n",
      "[tensor([76221], device='cuda:0'), tensor([8030], device='cuda:0'), tensor([72977], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "# Verify tensors are created\n",
    "print(torch_train_trigrams_input[:3])\n",
    "print(torch_train_trigrams_output[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "T-5cE7xATJit"
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2 \n",
    "EMBEDDING_DIM = 192 \n",
    "HIDDEN_DIM = 360 \n",
    "LEARNING_RATE = 0.01 #0.1 SGD learning rate\n",
    "if optimizer_choice == '2': # Adam learning rate\n",
    "  LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "DROPOUT_RATE = 0.3\n",
    "LOG_INTERVAL = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xUx9mcSpPvUJ"
   },
   "outputs": [],
   "source": [
    "# Define Feedforward Neural Network Language Model consisting of\n",
    "# 1.Input,\n",
    "# 2.Projection layer,\n",
    "# 3.Hidden layer\n",
    "# 4.Output layer.\n",
    "class FFNNLM(nn.Module):\n",
    "  def __init__(self, vocabulary_size, embedding_dim, context_size):\n",
    "    super(FFNNLM,self).__init__()\n",
    "    self.embeddings = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "    self.linear1 = nn.Linear(context_size * embedding_dim * BATCH_SIZE, HIDDEN_DIM)\n",
    "    self.linear2 = nn.Linear(HIDDEN_DIM, vocabulary_size)\n",
    "    self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    # Using Dropout for Projection and Hidden layer\n",
    "    # Prepare embeddings of projection layer for next layer's input\n",
    "    embeddings = self.dropout(self.embeddings(inputs).view((1,-1)))\n",
    "    # Use RelU activation function for output of hidden layer to be used on next layer\n",
    "    out = self.dropout(F.relu(self.linear1(embeddings)))\n",
    "    # Output layer\n",
    "    out = self.linear2(out)\n",
    "    # Compute log probabilities from the output layer\n",
    "    log_probs = F.log_softmax(out, dim=1) \n",
    "    \n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "o_VWnw0iTwno"
   },
   "outputs": [],
   "source": [
    "# Defines the language model from FFNNLM\n",
    "def createLanguageModel():\n",
    "  global losses \n",
    "  losses = []\n",
    "  global lossFunction \n",
    "  #Negative Log likelihood loss\n",
    "  lossFunction = nn.NLLLoss() \n",
    "  global language_model \n",
    "  # Initialize the language model\n",
    "  language_model = FFNNLM(len(train_vocabulary), EMBEDDING_DIM, CONTEXT_SIZE) \n",
    "  global optimizer\n",
    "  # Add Stochastic gradient descent optimizer\n",
    "  if (optimizer_choice == '2'): \n",
    "    optimizer = optim.AdamW(language_model.parameters())\n",
    "  else : # Default is SGD if user did not make choice\n",
    "    optimizer = optim.SGD(language_model.parameters(),lr=LEARNING_RATE, momentum=0.9, nesterov=True)\n",
    "    # Use learning rate scheduler\n",
    "    global scheduler\n",
    "    # step_size-Period of learning rate decay \n",
    "    # gamma-Multiplicative factor of learning rate decay\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.2)\n",
    "\n",
    "  language_model = language_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAUv0z5yUoPy",
    "outputId": "597e85ab-54ee-41fe-a6b4-38c43c79b6d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNNLM(\n",
      "  (embeddings): Embedding(81865, 192)\n",
      "  (linear1): Linear(in_features=98304, out_features=360, bias=True)\n",
      "  (linear2): Linear(in_features=360, out_features=81865, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Call the function to create the language model\n",
    "createLanguageModel()\n",
    "# Print the structure of the language model\n",
    "print(language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gkIOFlKzU_xd"
   },
   "outputs": [],
   "source": [
    "# Create a custom Dataset to be managed in DataLoader\n",
    "class Dataset(data.Dataset):\n",
    "    #constructor\n",
    "    def __init__(self, list_IDs, labels ):\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        context = self.list_IDs[index]\n",
    "        target = self.labels[index]\n",
    "\n",
    "        return context,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0v4Agk_CVorX"
   },
   "outputs": [],
   "source": [
    "# Initialize custom Dataset class to load train tensor data into batches\n",
    "training_set = Dataset(torch_train_trigrams_input, torch_train_trigrams_output)\n",
    "training_loader = torch.utils.data.DataLoader(training_set,batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNS5TO3AXHOX",
    "outputId": "1a3ab662-c652-49db-80e2-a07ff9a0a5c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set: 1110370\n",
      "Length of first batch: 4338\n",
      "First trigram: (tensor([39089, 34048], device='cuda:0'), tensor([76221], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# Check lenght of train set and first batch\n",
    "print(f\"Length of training set: {len(training_set)}\")\n",
    "print(f\"Length of first batch: {len(training_loader)}\")\n",
    "# Test the first trigram in the train set\n",
    "print(f\"First trigram: {next(iter(training_set))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "oec7enLIXPFJ"
   },
   "outputs": [],
   "source": [
    "# Prepare validation dataset the same as train above\n",
    "valid_sentence = clean_data(valid_document)\n",
    "# Handle unknown words\n",
    "valid_words, valid_set, valid_words_dict = unknownWords(valid_sentence)\n",
    "# Create training vocabulary\n",
    "valid_vocabulary = list(set(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8es-uEBaul1",
    "outputId": "ae69bae1-9c33-4079-fa8e-4dee38536de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ValidSet: 107328\n",
      "Total Vocabulary: 9735\n",
      "ValidSet UNK: 14212\n"
     ]
    }
   ],
   "source": [
    "print('Total ValidSet: %d' % len(valid_set))\n",
    "print('Total Vocabulary: %d' % len(set(valid_vocabulary)))\n",
    "print(f\"ValidSet UNK: {valid_set.count('UNK')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zVzl_VdubWqV"
   },
   "outputs": [],
   "source": [
    "# Create word-index mapping vocabulary for constructing context\n",
    "valid_word_to_index={valid_word: i for i, valid_word in enumerate(valid_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "NfBCZ8hub36M"
   },
   "outputs": [],
   "source": [
    "# Build the trigrams\n",
    "torch_valid_trigrams_input, torch_valid_trigrams_output = buid_trigrams(valid_words, valid_word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lz0QivyacG7q",
    "outputId": "171a53f3-6676-4055-b3d7-23c70954e008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([2549, 5068], device='cuda:0'), tensor([5068, 9218], device='cuda:0'), tensor([9218, 9218], device='cuda:0')]\n",
      "[tensor([9218], device='cuda:0'), tensor([9218], device='cuda:0'), tensor([8362], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "# Verify trigram tensors are created\n",
    "print(torch_valid_trigrams_input[:3])\n",
    "print(torch_valid_trigrams_output[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "M7JmgQ35c2b0"
   },
   "outputs": [],
   "source": [
    "# Initialize custom Dataset class to load validation tensor data into batches\n",
    "validation_set = Dataset(torch_valid_trigrams_input, torch_valid_trigrams_output)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set,batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-ocl5Icdfv6",
    "outputId": "e8ceff84-be5f-42aa-be58-7d4982beb609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of validation set: 97274\n",
      "Length of first batch: 380\n",
      "Validation First trigram: (tensor([2549, 5068], device='cuda:0'), tensor([9218], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# Check lenght of validation set and first batch\n",
    "print(f\"Length of validation set: {len(validation_set)}\")\n",
    "print(f\"Length of first batch: {len(validation_loader)}\")\n",
    "# Test the first trigram in the train set\n",
    "print(f\"Validation First trigram: {next(iter(validation_set))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "59sGDCf8dp2B"
   },
   "outputs": [],
   "source": [
    "# Prepare Test dataset the same as travalidationin above\n",
    "test_sentence = clean_data(test_document)\n",
    "# Handle unknown words\n",
    "test_words, test_set, test_words_dict = unknownWords(test_sentence)\n",
    "# Create training vocabulary\n",
    "test_vocabulary = list(set(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAqRC5dpilBJ",
    "outputId": "ce367e3e-272a-46be-f37d-0c72b512dbc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total TestSet: 81904\n",
      "Total Vocabulary: 8947\n",
      "TestSet UNK: 16431\n"
     ]
    }
   ],
   "source": [
    "print('Total TestSet: %d' % len(test_set))\n",
    "print('Total Vocabulary: %d' % len(set(test_vocabulary)))\n",
    "print(f\"TestSet UNK: {test_set.count('UNK')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "rhpTb2-si6ak"
   },
   "outputs": [],
   "source": [
    "# Create word-index mapping vocabulary for constructing context\n",
    "test_word_to_index={test_word: i for i, test_word in enumerate(test_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "KgwJtAk-jDSL"
   },
   "outputs": [],
   "source": [
    "# Build the trigrams\n",
    "torch_test_trigrams_input, torch_test_trigrams_output = buid_trigrams(test_words, test_word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVLkyaYhjMli",
    "outputId": "bcf5acbb-0dd1-4752-ff61-4eee86d05890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([4692, 4643], device='cuda:0'), tensor([4643, 3729], device='cuda:0'), tensor([3729, 1178], device='cuda:0')]\n",
      "[tensor([3729], device='cuda:0'), tensor([1178], device='cuda:0'), tensor([8495], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "# Verify trigram tensors are created\n",
    "print(torch_test_trigrams_input[:3])\n",
    "print(torch_test_trigrams_output[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "EqaPmGarjVXy"
   },
   "outputs": [],
   "source": [
    "# Initialize custom Dataset class to load test tensor data into batches\n",
    "testing_set = Dataset(torch_test_trigrams_input, torch_test_trigrams_output)\n",
    "testing_loader = torch.utils.data.DataLoader(testing_set,batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVrrRtJojlCh",
    "outputId": "544442fc-7638-4756-c3c9-f75634888aab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Testing set: 71832\n",
      "Length of first batch: 281\n",
      "Testing First trigram: (tensor([4692, 4643], device='cuda:0'), tensor([3729], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# Check lenght of validation set and first batch\n",
    "print(f\"Length of Testing set: {len(testing_set)}\")\n",
    "print(f\"Length of first batch: {len(testing_loader)}\")\n",
    "# Test the first trigram in the train set\n",
    "print(f\"Testing First trigram: {next(iter(testing_set))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "D-yTXp9M2L6u"
   },
   "outputs": [],
   "source": [
    "# Create a log file for model training\n",
    "global file\n",
    "file = open(\"log_file.txt\", \"w\")\n",
    "\n",
    "# Variables for ploting the graph\n",
    "global x_epoch\n",
    "global epoch_train_loss\n",
    "global validation_loss\n",
    "x_epoch = []\n",
    "epoch_train_loss = []\n",
    "validation_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "aHKx4yZmh8ez"
   },
   "outputs": [],
   "source": [
    "# Train the model and run validation after each epoch\n",
    "def training(epoch):\n",
    "    # Update learning rate\n",
    "    global LEARNING_RATE\n",
    "    if optimizer_choice == '1':\n",
    "      LEARNING_RATE = scheduler.get_last_lr()\n",
    "      LEARNING_RATE = LEARNING_RATE[0]\n",
    "    else:\n",
    "      LEARNING_RATE = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    # Set the model in training mode using test set\n",
    "    language_model.train()\n",
    "    print(\"Start Training\")\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"Start Training\")\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    global epoch_train_loss\n",
    "    accum_train_loss = 0\n",
    "\n",
    "    total_loss = 0\n",
    "    one_batch_count = 0\n",
    "    batch_loss = 0\n",
    "    batch_time = 0\n",
    "    for local_batch,local_target in training_loader:\n",
    "      if(len(local_batch)==BATCH_SIZE):\n",
    "        before_time = time.time()\n",
    "        local_loss = 0\n",
    "\n",
    "        loc_batch, loc_target = local_batch.to(device), local_target.to(device)\n",
    "\n",
    "        # Sets gradients of model parameters to zero, needs to be done for each epoch\n",
    "        language_model.zero_grad()\n",
    "        # Run forward pass and generates the log probabilities\n",
    "        log_probs = language_model(loc_batch)\n",
    "        \n",
    "        local_target = local_target.view(BATCH_SIZE, -1)\n",
    "        loss = lossFunction(log_probs, loc_target[len(loc_target)-1])\n",
    "        \n",
    "        #Backpropogation algorithm to update the gradients\n",
    "        loss.backward() \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        local_loss += loss.item()\n",
    "        \n",
    "        after_time = time.time()\n",
    "        batch_time += (after_time - before_time)\n",
    "\n",
    "        local_loss_out = local_loss\n",
    "        batch_loss += local_loss_out\n",
    "        total_loss += local_loss_out\n",
    "\n",
    "        accum_train_loss += local_loss * len(local_batch)\n",
    "\n",
    "        one_batch_count += 1\n",
    "\n",
    "        if (one_batch_count % LOG_INTERVAL == 0):\n",
    "          curr_loss = batch_loss / LOG_INTERVAL\n",
    "          print('| epoch {:3d} | {:4d}/{:4d} batches | lr {:02.6f} | ms/batch {:5.2f} | ''loss {:5.4f} | ppl {:8.2f}'.\n",
    "                format(\n",
    "                  epoch, one_batch_count, len(training_loader) // 1, LEARNING_RATE,\n",
    "                  batch_time/LOG_INTERVAL*1000, curr_loss, math.exp(curr_loss))\n",
    "                )\n",
    "          file.write('| epoch {:3d} | {:4d}/{:4d} batches | lr {:02.5f} | ms/batch {:5.2f} | ''loss {:5.4f} | ppl {:8.2f}'.\n",
    "                format(\n",
    "                  epoch, one_batch_count, len(training_loader) // 1, LEARNING_RATE,\n",
    "                  batch_time/LOG_INTERVAL*1000, curr_loss, math.exp(curr_loss))\n",
    "                )\n",
    "          file.write(\"\\n\")\n",
    "          batch_loss = 0\n",
    "          batch_time = 0\n",
    "\n",
    "    # Decay Learning Rate\n",
    "    if optimizer_choice == '1':\n",
    "      scheduler.step()\n",
    "\n",
    "    epoch_train_loss.append(accum_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Q1O5KvDUh6eY"
   },
   "outputs": [],
   "source": [
    "# Define the validation method used during training to validate the learning process\n",
    "# calculate loss and updates gradients\n",
    "def validation(epoch,train_time):\n",
    "  # Set the model in evaluation mode using validation set\n",
    "  language_model.eval()\n",
    "\n",
    "  global validation_loss\n",
    "  accum_validation_loss = 0\n",
    "  \n",
    "  total_loss = 0\n",
    "  one_batch_count = 0\n",
    "  batch_loss = 0\n",
    "  # Turn-off gradients calculations\n",
    "  with torch.set_grad_enabled(False):\n",
    "    for local_batch,local_target in validation_loader:\n",
    "      if(len(local_batch)==BATCH_SIZE):\n",
    "        local_loss=0\n",
    "        loc_batch, loc_target = local_batch.to(device), local_target.to(device)\n",
    "\n",
    "        # Run forward pass and generates the log probabilities\n",
    "        log_probs = language_model(loc_batch)\n",
    "\n",
    "        local_target = local_target.view(BATCH_SIZE, -1)\n",
    "        # Calculate loss based on what was predicted and what the next word is\n",
    "        loss = lossFunction(log_probs, loc_target[len(loc_target)-1])\n",
    "\n",
    "        local_loss += loss.item()\n",
    "        \n",
    "        local_loss_out = local_loss\n",
    "        batch_loss += local_loss_out\n",
    "        total_loss += local_loss_out\n",
    "\n",
    "        accum_validation_loss += local_loss * len(local_batch)\n",
    "\n",
    "        one_batch_count += 1\n",
    "\n",
    "        curr_loss = batch_loss/one_batch_count\n",
    "\n",
    "  file.write(\"-----------------------------------------------------------------------------------------\")\n",
    "  file.write(\"\\n\")\n",
    "  print('-----------------------------------------------------------------------------------------')\n",
    "  print('|end of epoch {:3d} | time {:5.2f}s| ''valid loss {:5.4f} | valid ppl {:8.2f}'.format(\n",
    "      epoch,train_time, curr_loss, math.exp(curr_loss))\n",
    "      )\n",
    "  file.write('|end of epoch {:3d} | time {:5.2f}s| ''valid loss {:5.4f} | valid ppl {:8.2f}'.format(\n",
    "      epoch,train_time, curr_loss, math.exp(curr_loss))\n",
    "      )\n",
    "  file.write(\"\\n\")\n",
    "  print('-----------------------------------------------------------------------------------------')\n",
    "  file.write(\"-----------------------------------------------------------------------------------------\")\n",
    "  batch_loss = 0\n",
    "  validation_loss.append(accum_validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "UYEhWT8dTr3F"
   },
   "outputs": [],
   "source": [
    "def testing(epoch = 0):\n",
    "  # Set the model in evaluation mode using testing set\n",
    "  language_model.eval()\n",
    "\n",
    "  total_loss = 0\n",
    "  one_batch_count = 0\n",
    "  batch_loss = 0\n",
    "  with torch.set_grad_enabled(False):\n",
    "      for local_batch,local_target in testing_loader:\n",
    "          if(len(local_batch)==BATCH_SIZE):\n",
    "              local_loss=0\n",
    "              loc_batch, loc_target = local_batch.to(device), local_target.to(device)\n",
    "\n",
    "              # Run forward pass and generates the log probabilities\n",
    "              log_probs = language_model(loc_batch)\n",
    "\n",
    "              local_target = local_target.view(BATCH_SIZE, -1)\n",
    "              # Calculate loss based on what was predicted and what the next word is\n",
    "              loss = lossFunction(log_probs, loc_target[len(loc_target)-1])\n",
    "\n",
    "              local_loss += loss.item()\n",
    "\n",
    "\n",
    "              local_loss_out = local_loss\n",
    "              batch_loss += local_loss_out\n",
    "              total_loss += local_loss_out\n",
    "              one_batch_count += 1\n",
    "\n",
    "              curr_loss = batch_loss/one_batch_count\n",
    "\n",
    "  file.write('========================================================================================')\n",
    "  file.write(\"\\n\")\n",
    "  print('========================================================================================')\n",
    "  print('|End of training | test loss {:5.4f} | test ppl {:8.2f}'.format(\n",
    "      curr_loss, math.exp(curr_loss))\n",
    "      )\n",
    "  file.write('|End of training | test loss {:5.4f} | test ppl {:8.2f}'.format(\n",
    "    curr_loss, math.exp(curr_loss))\n",
    "    )\n",
    "  file.write(\"\\n\")\n",
    "  print('========================================================================================')\n",
    "  file.write('========================================================================================')\n",
    "  file.write(\"\\n\")\n",
    "\n",
    "  batch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "DBCYe3DwovsI"
   },
   "outputs": [],
   "source": [
    "def draw_curve(epoch, train_loss, val_loss):\n",
    "  print(f\"x: {epoch}\")\n",
    "  print(f\"train: {train_loss}\")\n",
    "  print(f\"val: {val_loss}\")\n",
    "\n",
    "  x1_epoch = epoch \n",
    "  plt.plot(np.array(x1_epoch), np.array(train_loss), label='train')\n",
    "  x2_epoch = epoch\n",
    "  plt.plot(np.array(x2_epoch), np.array(val_loss), label='validation')\n",
    "  \n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Average Loss\")\n",
    "  # Set a title of the current axes.\n",
    "  plt.title('Average Train and Validation Loss per Epoch')\n",
    "  # show a legend on the plot\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "srlrEIUcOGl4"
   },
   "outputs": [],
   "source": [
    "def draw_train_curve(epoch, val_loss):\n",
    "    plt.plot(np.array(epoch), np.array(val_loss), label='train')\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    # Set a title of the current axes.\n",
    "    plt.title('Average Train Loss per Epoch')\n",
    "    # show a legend on the plot\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "-L9jLs9_OHno"
   },
   "outputs": [],
   "source": [
    "def draw_val_curve(epoch, train_loss):\n",
    "    plt.plot(np.array(epoch), np.array(train_loss), label='validation')\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    # Set a title of the current axes.\n",
    "    plt.title('Average Validation Loss per Epoch')\n",
    "    # show a legend on the plot\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iKvHMdfY1sC",
    "outputId": "97fcae34-1aa9-43de-ea4e-1007f821178c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "| epoch   0 |  200/4338 batches | lr 0.001000 | ms/batch 52.23 | loss 13.2437 | ppl 564528.10\n",
      "| epoch   0 |  400/4338 batches | lr 0.001000 | ms/batch 38.61 | loss 12.8769 | ppl 391172.01\n",
      "| epoch   0 |  600/4338 batches | lr 0.001000 | ms/batch 38.64 | loss 11.1749 | ppl 71315.07\n",
      "| epoch   0 |  800/4338 batches | lr 0.001000 | ms/batch 38.53 | loss 11.3060 | ppl 81311.98\n",
      "| epoch   0 | 1000/4338 batches | lr 0.001000 | ms/batch 38.50 | loss 15.3129 | ppl 4469786.34\n",
      "| epoch   0 | 1200/4338 batches | lr 0.001000 | ms/batch 38.46 | loss 11.8691 | ppl 142780.36\n",
      "| epoch   0 | 1400/4338 batches | lr 0.001000 | ms/batch 38.44 | loss 12.2865 | ppl 216745.04\n",
      "| epoch   0 | 1600/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 12.5791 | ppl 290421.93\n",
      "| epoch   0 | 1800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 11.2289 | ppl 75274.38\n",
      "| epoch   0 | 2000/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 11.0309 | ppl 61754.75\n",
      "| epoch   0 | 2200/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 10.9023 | ppl 54300.92\n",
      "| epoch   0 | 2400/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 11.5256 | ppl 101278.79\n",
      "| epoch   0 | 2600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 10.9969 | ppl 59686.16\n",
      "| epoch   0 | 2800/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 10.7383 | ppl 46086.63\n",
      "| epoch   0 | 3000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 10.7622 | ppl 47202.91\n",
      "| epoch   0 | 3200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 12.1579 | ppl 190600.39\n",
      "| epoch   0 | 3400/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 10.9045 | ppl 54421.95\n",
      "| epoch   0 | 3600/4338 batches | lr 0.001000 | ms/batch 38.42 | loss 11.2082 | ppl 73735.77\n",
      "| epoch   0 | 3800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 10.8875 | ppl 53504.43\n",
      "| epoch   0 | 4000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 10.5771 | ppl 39227.05\n",
      "| epoch   0 | 4200/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 10.6904 | ppl 43930.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch   0 | time 171.99s| valid loss 79.5676 | valid ppl 35954809440084280326405821416079360.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch   1 |  200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 0.8233 | ppl     2.28\n",
      "| epoch   1 |  400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 1.9034 | ppl     6.71\n",
      "| epoch   1 |  600/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.4619 | ppl  1740.43\n",
      "| epoch   1 |  800/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 8.5505 | ppl  5169.15\n",
      "| epoch   1 | 1000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 6.1268 | ppl   457.98\n",
      "| epoch   1 | 1200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 9.8607 | ppl 19162.29\n",
      "| epoch   1 | 1400/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 11.6429 | ppl 113879.44\n",
      "| epoch   1 | 1600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 10.2569 | ppl 28477.51\n",
      "| epoch   1 | 1800/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 10.2376 | ppl 27935.28\n",
      "| epoch   1 | 2000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 9.4333 | ppl 12497.34\n",
      "| epoch   1 | 2200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 8.8885 | ppl  7247.91\n",
      "| epoch   1 | 2400/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 11.5845 | ppl 107421.14\n",
      "| epoch   1 | 2600/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 9.3244 | ppl 11208.09\n",
      "| epoch   1 | 2800/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 9.1024 | ppl  8977.24\n",
      "| epoch   1 | 3000/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 9.2153 | ppl 10049.41\n",
      "| epoch   1 | 3200/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 12.3457 | ppl 229974.82\n",
      "| epoch   1 | 3400/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 12.0860 | ppl 177362.79\n",
      "| epoch   1 | 3600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 13.1003 | ppl 489091.63\n",
      "| epoch   1 | 3800/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 11.8950 | ppl 146530.02\n",
      "| epoch   1 | 4000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 10.2569 | ppl 28479.22\n",
      "| epoch   1 | 4200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 13.0205 | ppl 451563.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch   1 | time 169.05s| valid loss 83.3515 | valid ppl 1581609916661621383092585168807919616.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch   2 |  200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 2.5895 | ppl    13.32\n",
      "| epoch   2 |  400/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 4.2076 | ppl    67.20\n",
      "| epoch   2 |  600/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 11.9070 | ppl 148304.90\n",
      "| epoch   2 |  800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 9.2299 | ppl 10197.95\n",
      "| epoch   2 | 1000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 10.5202 | ppl 37055.80\n",
      "| epoch   2 | 1200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 12.2666 | ppl 212482.45\n",
      "| epoch   2 | 1400/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 12.8386 | ppl 376455.78\n",
      "| epoch   2 | 1600/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 9.9112 | ppl 20154.66\n",
      "| epoch   2 | 1800/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 8.4423 | ppl  4639.41\n",
      "| epoch   2 | 2000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.7928 | ppl  2423.20\n",
      "| epoch   2 | 2200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 8.2161 | ppl  3700.00\n",
      "| epoch   2 | 2400/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 12.3487 | ppl 230655.75\n",
      "| epoch   2 | 2600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 8.9325 | ppl  7574.03\n",
      "| epoch   2 | 2800/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 7.8878 | ppl  2664.63\n",
      "| epoch   2 | 3000/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 8.2194 | ppl  3712.40\n",
      "| epoch   2 | 3200/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 9.7480 | ppl 17120.08\n",
      "| epoch   2 | 3400/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 10.3124 | ppl 30103.82\n",
      "| epoch   2 | 3600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 12.6029 | ppl 297429.51\n",
      "| epoch   2 | 3800/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 9.8943 | ppl 19816.68\n",
      "| epoch   2 | 4000/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 9.1248 | ppl  9180.42\n",
      "| epoch   2 | 4200/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 10.3113 | ppl 30070.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch   2 | time 169.00s| valid loss 78.7877 | valid ppl 16483933558925656872099811508092928.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch   3 |  200/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 5.8977 | ppl   364.21\n",
      "| epoch   3 |  400/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.0320 | ppl  1132.33\n",
      "| epoch   3 |  600/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 10.7478 | ppl 46528.78\n",
      "| epoch   3 |  800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 8.7002 | ppl  6003.97\n",
      "| epoch   3 | 1000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 8.1714 | ppl  3538.17\n",
      "| epoch   3 | 1200/4338 batches | lr 0.001000 | ms/batch 38.42 | loss 11.2099 | ppl 73856.98\n",
      "| epoch   3 | 1400/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 10.6665 | ppl 42896.25\n",
      "| epoch   3 | 1600/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 9.3557 | ppl 11564.99\n",
      "| epoch   3 | 1800/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.5439 | ppl  1889.21\n",
      "| epoch   3 | 2000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.5752 | ppl  1949.19\n",
      "| epoch   3 | 2200/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 8.4269 | ppl  4568.54\n",
      "| epoch   3 | 2400/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 10.8918 | ppl 53736.14\n",
      "| epoch   3 | 2600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 9.2691 | ppl 10604.82\n",
      "| epoch   3 | 2800/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.4041 | ppl  1642.76\n",
      "| epoch   3 | 3000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.7584 | ppl  2341.08\n",
      "| epoch   3 | 3200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 9.2613 | ppl 10522.60\n",
      "| epoch   3 | 3400/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 10.9158 | ppl 55038.34\n",
      "| epoch   3 | 3600/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 9.9849 | ppl 21696.43\n",
      "| epoch   3 | 3800/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 10.4595 | ppl 34873.40\n",
      "| epoch   3 | 4000/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 7.9523 | ppl  2842.13\n",
      "| epoch   3 | 4200/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 9.8521 | ppl 18998.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch   3 | time 169.02s| valid loss 73.1207 | valid ppl 57006469539218025369556125483008.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch   4 |  200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.3291 | ppl  1524.03\n",
      "| epoch   4 |  400/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 9.1562 | ppl  9473.20\n",
      "| epoch   4 |  600/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 10.1756 | ppl 26255.82\n",
      "| epoch   4 |  800/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 10.2836 | ppl 29250.23\n",
      "| epoch   4 | 1000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 8.6910 | ppl  5949.37\n",
      "| epoch   4 | 1200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 8.5204 | ppl  5015.90\n",
      "| epoch   4 | 1400/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 10.9374 | ppl 56243.10\n",
      "| epoch   4 | 1600/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 10.0639 | ppl 23479.87\n",
      "| epoch   4 | 1800/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 7.7015 | ppl  2211.59\n",
      "| epoch   4 | 2000/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 7.6673 | ppl  2137.35\n",
      "| epoch   4 | 2200/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 6.9978 | ppl  1094.20\n",
      "| epoch   4 | 2400/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.5782 | ppl  5314.36\n",
      "| epoch   4 | 2600/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.5862 | ppl  1970.75\n",
      "| epoch   4 | 2800/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.2503 | ppl  1408.46\n",
      "| epoch   4 | 3000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.2467 | ppl  1403.43\n",
      "| epoch   4 | 3200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.1832 | ppl  3580.33\n",
      "| epoch   4 | 3400/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 10.3358 | ppl 30817.44\n",
      "| epoch   4 | 3600/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 9.1516 | ppl  9429.12\n",
      "| epoch   4 | 3800/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 8.9468 | ppl  7683.51\n",
      "| epoch   4 | 4000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.8910 | ppl  7266.07\n",
      "| epoch   4 | 4200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.9414 | ppl  7641.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch   4 | time 169.03s| valid loss 67.3190 | valid ppl 172297893367906701946436714496.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch   5 |  200/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.2245 | ppl   504.97\n",
      "| epoch   5 |  400/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 10.7556 | ppl 46890.43\n",
      "| epoch   5 |  600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 11.3680 | ppl 86511.85\n",
      "| epoch   5 |  800/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.4310 | ppl  4587.18\n",
      "| epoch   5 | 1000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 9.6140 | ppl 14973.28\n",
      "| epoch   5 | 1200/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 10.1677 | ppl 26048.74\n",
      "| epoch   5 | 1400/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 10.0342 | ppl 22792.47\n",
      "| epoch   5 | 1600/4338 batches | lr 0.001000 | ms/batch 38.43 | loss 8.9370 | ppl  7608.49\n",
      "| epoch   5 | 1800/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 6.9286 | ppl  1021.02\n",
      "| epoch   5 | 2000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.6222 | ppl  2042.96\n",
      "| epoch   5 | 2200/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 6.8587 | ppl   952.12\n",
      "| epoch   5 | 2400/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.1740 | ppl  3547.48\n",
      "| epoch   5 | 2600/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.3172 | ppl  1505.92\n",
      "| epoch   5 | 2800/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 7.1370 | ppl  1257.61\n",
      "| epoch   5 | 3000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.0630 | ppl  1167.90\n",
      "| epoch   5 | 3200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.1052 | ppl  3311.59\n",
      "| epoch   5 | 3400/4338 batches | lr 0.001000 | ms/batch 38.42 | loss 7.8089 | ppl  2462.54\n",
      "| epoch   5 | 3600/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.3054 | ppl  4045.47\n",
      "| epoch   5 | 3800/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.2725 | ppl  3914.82\n",
      "| epoch   5 | 4000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.6413 | ppl  2082.38\n",
      "| epoch   5 | 4200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.7398 | ppl  6246.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch   5 | time 169.12s| valid loss 60.4694 | valid ppl 182618726592379272910864384.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch   6 |  200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 6.8915 | ppl   983.84\n",
      "| epoch   6 |  400/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 10.9823 | ppl 58824.34\n",
      "| epoch   6 |  600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 9.6851 | ppl 16077.05\n",
      "| epoch   6 |  800/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 8.7894 | ppl  6564.31\n",
      "| epoch   6 | 1000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.5245 | ppl  5036.91\n",
      "| epoch   6 | 1200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 8.4790 | ppl  4812.64\n",
      "| epoch   6 | 1400/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 9.3059 | ppl 11002.54\n",
      "| epoch   6 | 1600/4338 batches | lr 0.001000 | ms/batch 38.43 | loss 9.8928 | ppl 19787.88\n",
      "| epoch   6 | 1800/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.8778 | ppl  2638.08\n",
      "| epoch   6 | 2000/4338 batches | lr 0.001000 | ms/batch 38.42 | loss 6.7929 | ppl   891.53\n",
      "| epoch   6 | 2200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 6.8017 | ppl   899.36\n",
      "| epoch   6 | 2400/4338 batches | lr 0.001000 | ms/batch 38.42 | loss 7.1505 | ppl  1274.76\n",
      "| epoch   6 | 2600/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 7.3277 | ppl  1521.89\n",
      "| epoch   6 | 2800/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.4159 | ppl  1662.16\n",
      "| epoch   6 | 3000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.3005 | ppl  1481.01\n",
      "| epoch   6 | 3200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 9.0876 | ppl  8845.26\n",
      "| epoch   6 | 3400/4338 batches | lr 0.001000 | ms/batch 38.42 | loss 8.9478 | ppl  7690.62\n",
      "| epoch   6 | 3600/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 8.1953 | ppl  3623.92\n",
      "| epoch   6 | 3800/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 9.2366 | ppl 10266.49\n",
      "| epoch   6 | 4000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.0011 | ppl  2984.28\n",
      "| epoch   6 | 4200/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 9.0212 | ppl  8276.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch   6 | time 169.12s| valid loss 52.0448 | valid ppl 40063875872839014809600.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch   7 |  200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 9.3311 | ppl 11283.76\n",
      "| epoch   7 |  400/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 9.1531 | ppl  9443.98\n",
      "| epoch   7 |  600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 8.5572 | ppl  5204.30\n",
      "| epoch   7 |  800/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 9.6321 | ppl 15245.72\n",
      "| epoch   7 | 1000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 10.2316 | ppl 27766.27\n",
      "| epoch   7 | 1200/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 8.0147 | ppl  3024.99\n",
      "| epoch   7 | 1400/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 8.0063 | ppl  2999.72\n",
      "| epoch   7 | 1600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 8.7930 | ppl  6588.10\n",
      "| epoch   7 | 1800/4338 batches | lr 0.001000 | ms/batch 38.46 | loss 7.0046 | ppl  1101.69\n",
      "| epoch   7 | 2000/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.0254 | ppl  1124.80\n",
      "| epoch   7 | 2200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 6.7821 | ppl   881.93\n",
      "| epoch   7 | 2400/4338 batches | lr 0.001000 | ms/batch 38.42 | loss 8.4857 | ppl  4845.19\n",
      "| epoch   7 | 2600/4338 batches | lr 0.001000 | ms/batch 38.48 | loss 7.2771 | ppl  1446.85\n",
      "| epoch   7 | 2800/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 7.8144 | ppl  2476.02\n",
      "| epoch   7 | 3000/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 7.4275 | ppl  1681.60\n",
      "| epoch   7 | 3200/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 8.5151 | ppl  4989.73\n",
      "| epoch   7 | 3400/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 8.6000 | ppl  5431.66\n",
      "| epoch   7 | 3600/4338 batches | lr 0.001000 | ms/batch 38.40 | loss 7.7519 | ppl  2325.99\n",
      "| epoch   7 | 3800/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 8.3037 | ppl  4038.80\n",
      "| epoch   7 | 4000/4338 batches | lr 0.001000 | ms/batch 38.41 | loss 7.5127 | ppl  1831.12\n",
      "| epoch   7 | 4200/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 7.6183 | ppl  2035.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch   7 | time 169.10s| valid loss 40.3509 | valid ppl 334337659404753536.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch   8 |  200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.0521 | ppl   424.99\n",
      "| epoch   8 |  400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 8.8892 | ppl  7253.36\n",
      "| epoch   8 |  600/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 8.1415 | ppl  3434.05\n",
      "| epoch   8 |  800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.4679 | ppl  1751.00\n",
      "| epoch   8 | 1000/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 10.3147 | ppl 30171.64\n",
      "| epoch   8 | 1200/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 8.1053 | ppl  3312.05\n",
      "| epoch   8 | 1400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 8.2375 | ppl  3779.94\n",
      "| epoch   8 | 1600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 8.1995 | ppl  3639.05\n",
      "| epoch   8 | 1800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.0208 | ppl  1119.65\n",
      "| epoch   8 | 2000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.0337 | ppl  1134.24\n",
      "| epoch   8 | 2200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.9858 | ppl  1081.21\n",
      "| epoch   8 | 2400/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.1890 | ppl  1324.71\n",
      "| epoch   8 | 2600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.1316 | ppl  1250.84\n",
      "| epoch   8 | 2800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.0596 | ppl  1163.98\n",
      "| epoch   8 | 3000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.0782 | ppl  1185.80\n",
      "| epoch   8 | 3200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 8.1140 | ppl  3340.78\n",
      "| epoch   8 | 3400/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 8.2613 | ppl  3871.17\n",
      "| epoch   8 | 3600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 8.5326 | ppl  5077.61\n",
      "| epoch   8 | 3800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.7383 | ppl  2294.50\n",
      "| epoch   8 | 4000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.4237 | ppl  1675.20\n",
      "| epoch   8 | 4200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.5611 | ppl  1921.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch   8 | time 168.39s| valid loss 34.6922 | valid ppl 1165868834572969.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch   9 |  200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.6354 | ppl  2070.10\n",
      "| epoch   9 |  400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 8.7139 | ppl  6086.81\n",
      "| epoch   9 |  600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 9.1099 | ppl  9044.61\n",
      "| epoch   9 |  800/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 7.0102 | ppl  1107.86\n",
      "| epoch   9 | 1000/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 9.4216 | ppl 12352.39\n",
      "| epoch   9 | 1200/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 8.5604 | ppl  5220.92\n",
      "| epoch   9 | 1400/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 8.5203 | ppl  5015.52\n",
      "| epoch   9 | 1600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 9.2552 | ppl 10458.31\n",
      "| epoch   9 | 1800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.1786 | ppl  1311.01\n",
      "| epoch   9 | 2000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.7664 | ppl   868.15\n",
      "| epoch   9 | 2200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 6.7698 | ppl   871.10\n",
      "| epoch   9 | 2400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 8.2488 | ppl  3822.89\n",
      "| epoch   9 | 2600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 8.4799 | ppl  4816.97\n",
      "| epoch   9 | 2800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.0556 | ppl  1159.38\n",
      "| epoch   9 | 3000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.1761 | ppl  1307.77\n",
      "| epoch   9 | 3200/4338 batches | lr 0.001000 | ms/batch 39.01 | loss 8.0482 | ppl  3128.16\n",
      "| epoch   9 | 3400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.4310 | ppl  1687.43\n",
      "| epoch   9 | 3600/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.7585 | ppl  2341.27\n",
      "| epoch   9 | 3800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.6210 | ppl  2040.56\n",
      "| epoch   9 | 4000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 8.2973 | ppl  4013.10\n",
      "| epoch   9 | 4200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.4987 | ppl  1805.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch   9 | time 168.63s| valid loss 29.6430 | valid ppl 7477900884053.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch  10 |  200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.4649 | ppl  1745.74\n",
      "| epoch  10 |  400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 9.3335 | ppl 11310.66\n",
      "| epoch  10 |  600/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 8.2542 | ppl  3843.86\n",
      "| epoch  10 |  800/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.7584 | ppl   861.22\n",
      "| epoch  10 | 1000/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 8.4715 | ppl  4776.75\n",
      "| epoch  10 | 1200/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 7.4573 | ppl  1732.51\n",
      "| epoch  10 | 1400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 8.7326 | ppl  6201.98\n",
      "| epoch  10 | 1600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.9420 | ppl  1034.81\n",
      "| epoch  10 | 1800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.9972 | ppl  1093.53\n",
      "| epoch  10 | 2000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.7880 | ppl   887.16\n",
      "| epoch  10 | 2200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.9264 | ppl  1018.82\n",
      "| epoch  10 | 2400/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 8.3055 | ppl  4046.05\n",
      "| epoch  10 | 2600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.1965 | ppl  1334.73\n",
      "| epoch  10 | 2800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.1422 | ppl  1264.22\n",
      "| epoch  10 | 3000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.1201 | ppl  1236.60\n",
      "| epoch  10 | 3200/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 7.5835 | ppl  1965.58\n",
      "| epoch  10 | 3400/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 7.3761 | ppl  1597.39\n",
      "| epoch  10 | 3600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 7.6708 | ppl  2144.82\n",
      "| epoch  10 | 3800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.9947 | ppl  2965.17\n",
      "| epoch  10 | 4000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.3420 | ppl  1543.76\n",
      "| epoch  10 | 4200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 8.3631 | ppl  4285.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch  10 | time 168.46s| valid loss 24.0930 | valid ppl 29072163964.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch  11 |  200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.7019 | ppl   813.93\n",
      "| epoch  11 |  400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.9027 | ppl   994.98\n",
      "| epoch  11 |  600/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.2200 | ppl  1366.52\n",
      "| epoch  11 |  800/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.6204 | ppl  2039.43\n",
      "| epoch  11 | 1000/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 7.6684 | ppl  2139.56\n",
      "| epoch  11 | 1200/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 10.2861 | ppl 29322.23\n",
      "| epoch  11 | 1400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 9.4393 | ppl 12572.55\n",
      "| epoch  11 | 1600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 8.8690 | ppl  7107.95\n",
      "| epoch  11 | 1800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.8420 | ppl   936.32\n",
      "| epoch  11 | 2000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.7609 | ppl   863.39\n",
      "| epoch  11 | 2200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.7455 | ppl   850.23\n",
      "| epoch  11 | 2400/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 8.1209 | ppl  3364.20\n",
      "| epoch  11 | 2600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 7.0936 | ppl  1204.19\n",
      "| epoch  11 | 2800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.0317 | ppl  1131.92\n",
      "| epoch  11 | 3000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.0292 | ppl  1129.18\n",
      "| epoch  11 | 3200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.9135 | ppl  2733.95\n",
      "| epoch  11 | 3400/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 7.5541 | ppl  1908.46\n",
      "| epoch  11 | 3600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.5466 | ppl  1894.37\n",
      "| epoch  11 | 3800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.5806 | ppl  1959.83\n",
      "| epoch  11 | 4000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.6032 | ppl  2004.65\n",
      "| epoch  11 | 4200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.5457 | ppl  1892.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch  11 | time 168.40s| valid loss 22.5212 | valid ppl 6036907671.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch  12 |  200/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 5.8219 | ppl   337.62\n",
      "| epoch  12 |  400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.9973 | ppl  2972.94\n",
      "| epoch  12 |  600/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.7804 | ppl  2393.20\n",
      "| epoch  12 |  800/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.8021 | ppl   899.74\n",
      "| epoch  12 | 1000/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 9.0355 | ppl  8396.14\n",
      "| epoch  12 | 1200/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 8.4779 | ppl  4807.37\n",
      "| epoch  12 | 1400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 8.4529 | ppl  4688.45\n",
      "| epoch  12 | 1600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.5941 | ppl  1986.37\n",
      "| epoch  12 | 1800/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.7585 | ppl   861.36\n",
      "| epoch  12 | 2000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.8413 | ppl   935.73\n",
      "| epoch  12 | 2200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.9501 | ppl  1043.21\n",
      "| epoch  12 | 2400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.1631 | ppl  1290.97\n",
      "| epoch  12 | 2600/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.5953 | ppl  1988.75\n",
      "| epoch  12 | 2800/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.0299 | ppl  1129.88\n",
      "| epoch  12 | 3000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.6788 | ppl   795.34\n",
      "| epoch  12 | 3200/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.3535 | ppl  1561.63\n",
      "| epoch  12 | 3400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.4639 | ppl  1744.02\n",
      "| epoch  12 | 3600/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.5223 | ppl  1848.78\n",
      "| epoch  12 | 3800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.9039 | ppl  2707.77\n",
      "| epoch  12 | 4000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.2843 | ppl  1457.21\n",
      "| epoch  12 | 4200/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.7943 | ppl  2426.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch  12 | time 168.44s| valid loss 20.2760 | valid ppl 639356456.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch  13 |  200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 4.9076 | ppl   135.31\n",
      "| epoch  13 |  400/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 5.8322 | ppl   341.11\n",
      "| epoch  13 |  600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.7530 | ppl  2328.51\n",
      "| epoch  13 |  800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.6643 | ppl   783.89\n",
      "| epoch  13 | 1000/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.8588 | ppl   952.27\n",
      "| epoch  13 | 1200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.8574 | ppl   950.92\n",
      "| epoch  13 | 1400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.9556 | ppl  2851.40\n",
      "| epoch  13 | 1600/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.2437 | ppl  1399.28\n",
      "| epoch  13 | 1800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.2614 | ppl  1424.18\n",
      "| epoch  13 | 2000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 6.7230 | ppl   831.32\n",
      "| epoch  13 | 2200/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.8986 | ppl   990.90\n",
      "| epoch  13 | 2400/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.9145 | ppl  2736.68\n",
      "| epoch  13 | 2600/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.9341 | ppl  1026.74\n",
      "| epoch  13 | 2800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.4316 | ppl  1688.48\n",
      "| epoch  13 | 3000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.8668 | ppl   959.89\n",
      "| epoch  13 | 3200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.2607 | ppl  1423.28\n",
      "| epoch  13 | 3400/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.3616 | ppl  1574.37\n",
      "| epoch  13 | 3600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.5057 | ppl  1818.33\n",
      "| epoch  13 | 3800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.7174 | ppl  2247.19\n",
      "| epoch  13 | 4000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 8.2240 | ppl  3729.26\n",
      "| epoch  13 | 4200/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.3969 | ppl  1630.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch  13 | time 168.48s| valid loss 18.9764 | valid ppl 174324396.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch  14 |  200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 3.4973 | ppl    33.03\n",
      "| epoch  14 |  400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.4713 | ppl   646.32\n",
      "| epoch  14 |  600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.4576 | ppl   637.51\n",
      "| epoch  14 |  800/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.8181 | ppl   914.23\n",
      "| epoch  14 | 1000/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 8.0125 | ppl  3018.39\n",
      "| epoch  14 | 1200/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 8.0588 | ppl  3161.44\n",
      "| epoch  14 | 1400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.1119 | ppl  1226.48\n",
      "| epoch  14 | 1600/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 7.4418 | ppl  1705.76\n",
      "| epoch  14 | 1800/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.0849 | ppl  1193.86\n",
      "| epoch  14 | 2000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 6.7427 | ppl   847.81\n",
      "| epoch  14 | 2200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.7923 | ppl   890.92\n",
      "| epoch  14 | 2400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.9219 | ppl  1014.28\n",
      "| epoch  14 | 2600/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.9790 | ppl  1073.82\n",
      "| epoch  14 | 2800/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.1540 | ppl  1279.15\n",
      "| epoch  14 | 3000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.7936 | ppl   892.09\n",
      "| epoch  14 | 3200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.7388 | ppl  2295.66\n",
      "| epoch  14 | 3400/4338 batches | lr 0.001000 | ms/batch 39.35 | loss 7.5386 | ppl  1879.29\n",
      "| epoch  14 | 3600/4338 batches | lr 0.001000 | ms/batch 38.69 | loss 7.4886 | ppl  1787.48\n",
      "| epoch  14 | 3800/4338 batches | lr 0.001000 | ms/batch 40.31 | loss 7.5907 | ppl  1979.74\n",
      "| epoch  14 | 4000/4338 batches | lr 0.001000 | ms/batch 38.79 | loss 7.2701 | ppl  1436.64\n",
      "| epoch  14 | 4200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.3711 | ppl  1589.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch  14 | time 169.36s| valid loss 18.0987 | valid ppl 72468719.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch  15 |  200/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 4.2772 | ppl    72.04\n",
      "| epoch  15 |  400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.8773 | ppl   969.97\n",
      "| epoch  15 |  600/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.8203 | ppl   916.23\n",
      "| epoch  15 |  800/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.5137 | ppl   674.30\n",
      "| epoch  15 | 1000/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.9404 | ppl  1033.18\n",
      "| epoch  15 | 1200/4338 batches | lr 0.001000 | ms/batch 39.10 | loss 6.7650 | ppl   866.99\n",
      "| epoch  15 | 1400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.7196 | ppl   828.48\n",
      "| epoch  15 | 1600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.9329 | ppl  1025.45\n",
      "| epoch  15 | 1800/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.8398 | ppl   934.27\n",
      "| epoch  15 | 2000/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 6.8018 | ppl   899.42\n",
      "| epoch  15 | 2200/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 6.7928 | ppl   891.43\n",
      "| epoch  15 | 2400/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 6.7790 | ppl   879.17\n",
      "| epoch  15 | 2600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.1321 | ppl  1251.49\n",
      "| epoch  15 | 2800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.1315 | ppl  1250.74\n",
      "| epoch  15 | 3000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.7045 | ppl   816.07\n",
      "| epoch  15 | 3200/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.2784 | ppl  1448.71\n",
      "| epoch  15 | 3400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 8.1253 | ppl  3378.91\n",
      "| epoch  15 | 3600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.4742 | ppl  1761.92\n",
      "| epoch  15 | 3800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.5267 | ppl  1856.98\n",
      "| epoch  15 | 4000/4338 batches | lr 0.001000 | ms/batch 38.76 | loss 7.2223 | ppl  1369.58\n",
      "| epoch  15 | 4200/4338 batches | lr 0.001000 | ms/batch 38.77 | loss 7.3684 | ppl  1585.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch  15 | time 168.79s| valid loss 17.2070 | valid ppl 29708901.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch  16 |  200/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 5.9714 | ppl   392.05\n",
      "| epoch  16 |  400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 5.7066 | ppl   300.85\n",
      "| epoch  16 |  600/4338 batches | lr 0.001000 | ms/batch 38.33 | loss 6.4628 | ppl   640.85\n",
      "| epoch  16 |  800/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.9762 | ppl  1070.86\n",
      "| epoch  16 | 1000/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 7.4265 | ppl  1679.99\n",
      "| epoch  16 | 1200/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.2423 | ppl  1397.25\n",
      "| epoch  16 | 1400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.3617 | ppl  1574.55\n",
      "| epoch  16 | 1600/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.1641 | ppl  1292.14\n",
      "| epoch  16 | 1800/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.8159 | ppl   912.28\n",
      "| epoch  16 | 2000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.7884 | ppl   887.52\n",
      "| epoch  16 | 2200/4338 batches | lr 0.001000 | ms/batch 39.13 | loss 6.7971 | ppl   895.24\n",
      "| epoch  16 | 2400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.0285 | ppl  1128.32\n",
      "| epoch  16 | 2600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.9642 | ppl  1058.04\n",
      "| epoch  16 | 2800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.0038 | ppl  1100.79\n",
      "| epoch  16 | 3000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.7021 | ppl   814.10\n",
      "| epoch  16 | 3200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.2626 | ppl  1425.94\n",
      "| epoch  16 | 3400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.5750 | ppl  1948.95\n",
      "| epoch  16 | 3600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 7.8995 | ppl  2695.90\n",
      "| epoch  16 | 3800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.4619 | ppl  1740.48\n",
      "| epoch  16 | 4000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.8577 | ppl  2585.46\n",
      "| epoch  16 | 4200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.3327 | ppl  1529.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch  16 | time 168.52s| valid loss 16.0663 | valid ppl 9495583.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch  17 |  200/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 5.4469 | ppl   232.05\n",
      "| epoch  17 |  400/4338 batches | lr 0.001000 | ms/batch 38.33 | loss 7.3964 | ppl  1630.05\n",
      "| epoch  17 |  600/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.5675 | ppl   711.57\n",
      "| epoch  17 |  800/4338 batches | lr 0.001000 | ms/batch 38.33 | loss 6.5952 | ppl   731.59\n",
      "| epoch  17 | 1000/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.4340 | ppl  1692.55\n",
      "| epoch  17 | 1200/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.7768 | ppl  2384.71\n",
      "| epoch  17 | 1400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.2880 | ppl  1462.71\n",
      "| epoch  17 | 1600/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.2719 | ppl  1439.35\n",
      "| epoch  17 | 1800/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.8240 | ppl   919.66\n",
      "| epoch  17 | 2000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.8603 | ppl   953.67\n",
      "| epoch  17 | 2200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.0980 | ppl  1209.56\n",
      "| epoch  17 | 2400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.9736 | ppl  1068.07\n",
      "| epoch  17 | 2600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.9640 | ppl  1057.84\n",
      "| epoch  17 | 2800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.0012 | ppl  1098.00\n",
      "| epoch  17 | 3000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.7508 | ppl   854.77\n",
      "| epoch  17 | 3200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.7134 | ppl  2238.15\n",
      "| epoch  17 | 3400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.2897 | ppl  1465.17\n",
      "| epoch  17 | 3600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.4566 | ppl  1731.32\n",
      "| epoch  17 | 3800/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 7.4691 | ppl  1753.03\n",
      "| epoch  17 | 4000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.5017 | ppl  1811.19\n",
      "| epoch  17 | 4200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.3220 | ppl  1513.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch  17 | time 168.36s| valid loss 15.7388 | valid ppl 6843372.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch  18 |  200/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 4.7176 | ppl   111.89\n",
      "| epoch  18 |  400/4338 batches | lr 0.001000 | ms/batch 38.33 | loss 5.8925 | ppl   362.32\n",
      "| epoch  18 |  600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.3428 | ppl   568.37\n",
      "| epoch  18 |  800/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.7944 | ppl   892.83\n",
      "| epoch  18 | 1000/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.3515 | ppl  1558.54\n",
      "| epoch  18 | 1200/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.0703 | ppl  1176.55\n",
      "| epoch  18 | 1400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.1349 | ppl  1255.06\n",
      "| epoch  18 | 1600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 7.0979 | ppl  1209.37\n",
      "| epoch  18 | 1800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.8482 | ppl   942.18\n",
      "| epoch  18 | 2000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.8089 | ppl   905.88\n",
      "| epoch  18 | 2200/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 6.9501 | ppl  1043.27\n",
      "| epoch  18 | 2400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.0953 | ppl  1206.27\n",
      "| epoch  18 | 2600/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.0175 | ppl  1115.99\n",
      "| epoch  18 | 2800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.9992 | ppl  1095.80\n",
      "| epoch  18 | 3000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.7047 | ppl   816.23\n",
      "| epoch  18 | 3200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.3118 | ppl  1497.81\n",
      "| epoch  18 | 3400/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.4334 | ppl  1691.50\n",
      "| epoch  18 | 3600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.6795 | ppl  2163.61\n",
      "| epoch  18 | 3800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 8.6715 | ppl  5834.52\n",
      "| epoch  18 | 4000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.1874 | ppl  1322.60\n",
      "| epoch  18 | 4200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.3057 | ppl  1488.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch  18 | time 168.37s| valid loss 15.7587 | valid ppl 6980827.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start Training\n",
      "| epoch  19 |  200/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 7.3598 | ppl  1571.59\n",
      "| epoch  19 |  400/4338 batches | lr 0.001000 | ms/batch 38.33 | loss 5.7718 | ppl   321.10\n",
      "| epoch  19 |  600/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.4401 | ppl   626.47\n",
      "| epoch  19 |  800/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.6465 | ppl   770.06\n",
      "| epoch  19 | 1000/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.6692 | ppl   787.80\n",
      "| epoch  19 | 1200/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.9699 | ppl  1064.08\n",
      "| epoch  19 | 1400/4338 batches | lr 0.001000 | ms/batch 38.34 | loss 6.8879 | ppl   980.35\n",
      "| epoch  19 | 1600/4338 batches | lr 0.001000 | ms/batch 38.35 | loss 6.8003 | ppl   898.13\n",
      "| epoch  19 | 1800/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.9792 | ppl  1074.08\n",
      "| epoch  19 | 2000/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.8132 | ppl   909.77\n",
      "| epoch  19 | 2200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.8080 | ppl   905.03\n",
      "| epoch  19 | 2400/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.8951 | ppl   987.38\n",
      "| epoch  19 | 2600/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 6.9641 | ppl  1057.97\n",
      "| epoch  19 | 2800/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 6.9984 | ppl  1094.85\n",
      "| epoch  19 | 3000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 6.8703 | ppl   963.19\n",
      "| epoch  19 | 3200/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.2474 | ppl  1404.47\n",
      "| epoch  19 | 3400/4338 batches | lr 0.001000 | ms/batch 38.39 | loss 7.3017 | ppl  1482.78\n",
      "| epoch  19 | 3600/4338 batches | lr 0.001000 | ms/batch 38.38 | loss 7.5332 | ppl  1868.99\n",
      "| epoch  19 | 3800/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.4854 | ppl  1781.86\n",
      "| epoch  19 | 4000/4338 batches | lr 0.001000 | ms/batch 38.37 | loss 7.2107 | ppl  1353.87\n",
      "| epoch  19 | 4200/4338 batches | lr 0.001000 | ms/batch 38.36 | loss 7.2958 | ppl  1474.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "|end of epoch  19 | time 168.39s| valid loss 15.0111 | valid ppl 3305412.62\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now, call the above functions in order to train model per epoch and validate model, printing the performance\n",
    "for epoch in range(EPOCHS):\n",
    "  x_epoch.append(epoch + 1)\n",
    "  # Run training\n",
    "  start_total_train_time=time.time()\n",
    "  training(epoch)\n",
    "  total_train_time = time.time() - start_total_train_time\n",
    "  # Run validation\n",
    "  validation(epoch, total_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "QFF3cEbfo5AG",
    "outputId": "87fafa1f-0d85-44ea-e15e-590f24994701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "train: [12883404.293452263, 10434965.111952223, 10439874.041120935, 9970918.242243497, 9657184.111127809, 9269274.619587142, 9275225.249968411, 9075566.159544023, 8599648.94018073, 8801859.180149471, 8475482.144144915, 8466556.039827198, 8271978.465109607, 7970243.3123464, 7811014.84212437, 7691833.9727465045, 7833375.582008186, 7894254.606715281, 7761378.3691465175, 7737006.97784676]\n",
      "val: [7719964.001464844, 8087096.577392578, 7644297.2470703125, 7094464.499755859, 6531560.83203125, 5866987.213378906, 5049590.9912109375, 3915008.4221191406, 3365980.1833496094, 2876079.8354492188, 2337603.791015625, 2185092.8088378906, 1967255.9743652344, 1841168.9956054688, 1756004.9289550781, 1669487.8229980469, 1558820.3103027344, 1527040.4780273438, 1528969.9792480469, 1456434.2373046875]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3wVddb48c9JI6SQTglJCAooUgMRsaC4WBAV7Oiqu1hX176ruz67+6w+7u7zW7f4uHZxxbZ2XMtiL9hF6UhTWiAhdAglCYQk5/fHdwKXmHJDcjNJ7nm/uK87fc6dDHNmvt+Z74iqYowxJnxF+B2AMcYYf1kiMMaYMGeJwBhjwpwlAmOMCXOWCIwxJsxZIjDGmDBnicCEhIhcLCLvtYE4ckVERSQqBMtWEenjdT8iIv8dzLQHsZ42sS1N/URktIgU+R3HwbJE0AJE5GMR2SYinfyOpTm8g9ku71MhInsD+t9uyrJU9VlVPSVUsbYEEXlHRO6qY/gEEVnflOShqteo6h9aIKYfJK5Qbcv2fvCqT8A23FXrM9Hv2NoqSwTNJCK5wChAgfEhWH6Ln8nWxzuYJahqAvC/wIs1/ap6mh8xhdhTwCUiIrWGXwo8q6qVPsRkmqCRfTE5YP9NUNUXWy2wdsYSQfP9BJgBPAn8FEBEOolIiYgMrJlIRDJEpFxEunr9Z4jIPG+6L0VkcMC0BSLyaxFZAJSKSJSI3C4iK0Rkp4gsFpGzA6aPFJG/i8hmEVklItcHnlWKSJKIPC4i60RkrYj8UUQim/IjDyKmSSLyeUC/isg1IrLM+80P1nEArpl2hIh85U23TkQeEJGYYJblbYu/edtiJXB6Az/rNSANl8hrlp0CnAE83VgctWJ+UkT+GNB/mzdPsYhcXmva00VkrojsEJFCEbkzYPSn3neJdxZ7dB3b8hgRmSki273vYwLGfSwifxCRL7y/y3sikt7ANqiTiPT3llUiIotEZHzAuHHe33untz/d6g1PF5Fp3jxbReQzEanzGOP9DW8UkZXe3+qvgdOKyOUiskTclfa7ItKr1rzXicgyYNlB/LYnxV39vu/9hk9qLb+h7ZsqIk94f9dtIvJarWX/UkQ2en/7y5oam29U1T7N+ADLgZ8Dw4G9QDdv+BTgTwHTXQe843XnARuBo4BIXAIpADp54wuAeUA20Nkbdj6QiUveE4FSoIc37hpgMZAFpAAf4K5QorzxrwKPAvFAV+Ab4GeN/K47gX8F9Dc1pknA5wHzKzANSAZygE3A2HrWPRwYCUQBucAS4OZgluVti6VenKnA9MBtUce6HgP+GdD/M2BeE+Lo43U/CfzR6x4LbAAGetv8uVrTjgYGedttsDftWd643NrxBm5L7zdtw121RAEXef1p3viPgRVAP6Cz1//nen77aKCojuHRuP36N0AM8CNgJ3CYN34dMMrrTgGGed3/D3jEmz8al2ClnnWr97dJ9f6G3wNXeuMmeOvv7/3G3wFf1pr3fW/eznUs+wfbsNb4J73fczzQCfhHE7bvm8CL3u+OBk4I2JaVwF3e8HFAGZDi9zEqqOOY3wEcVNDuILsRWBjEtP+HO4DN83a2khaM4zjcwT/d618K3OJ1nwSsCJj2C+AnXvfDwB9qLeu7gJ2qALi8kXXPAyZ43R8RcGD31q3ejtwN2BP4H8bbuac3svw7+WEiaEpMk/hhIjguoP8l4PYgt/PNwKvBLMvbFtcEjDulkYPCcUAJEBvwd7qlCXHUlQimEHDwxR2U901bx3LvBf7P686tHS8HJoJLgW9qzf8VMMnr/hj4XcC4n+OdgNSx3tHUnQhGAeuBiIBhzwN3et1rcAmzS6357gJer+931ppWCTgR8OL80Ot+G7giYFwE7qDaK2DeHzWw7JptWFLr0z/gb/VCwPQJQBXu5KHe7Qv0AKqp4+DubcvyWn+3jcDIYPZxvz/ttWjoSdxZV6NU9RZVHaqqQ4H7gX+3YBw/Bd5T1c1e/3PeMHBnO3EicpS4eoShuDNzgF7AL71L6BIRKcHthJkByy4MXJGI/ET2FyWV4M42ay75M2tNH9jdC3eGsi5g3kdxVwZN1ZSY6rI+oLsM9x/wB0Skn1fEsF5EduDqK2ovt75l1d4WqxuIB1X9HNgMnCUihwIjcH/HYOOoS4MxePvEdBHZJCLbcVcxwRbfZNbxm1YDPQP6g9rOjayjUFWr61nHubgz3tVescrR3vC/4s7k3/OKfG5vZD21t1HN/t8L+EfAfrUVEA78jQfsi/VIV9XkgM+SuuZX1V3eOjJpePtmA1tVdVs969uiB9YrHcy290W7TASq+inuD7ePiBwq7i6Q2V7Z5OF1zHoR7sym2USkM3ABcIJ3oFgP3AIMEZEhqlqFO1O9yPtMU9Wd3uyFuGKjwJ00TlUDY9OAdfXCFWFcj7tETQYW4v5zgLtUzwqYNzuguxB3RRD4n6KLqg44iJ/dlJia42Hc1VVfVe2CK6IIdrnrOPD35wQxz9O4up5LgHdVdUMz42gshueAN4BsVU3CFafULFdpWDHuQBkoB1gbRFzBKgaya5Xv71uHqs5U1Qm4k4nXcPs5qrpTVX+pqofgbpz4hYiMaWA9tbdRsdddiLvCDfz/0VlVvwyYvrHt1Jh96xaRBFyRUDENb99CIFVEkpu57janXSaCekwGblDV4cCtwEOBI70DV29c0UFLOAt3OXkE7mx/KK5M8zPcQQXcf/iJwMVed43HgGu8M0MRkXhxFYiJ9awrHrfjb/J+y2W4s+8aLwE3iUhPbyf9dc0IVV0HvAf8XUS6iEiElzRPaM6PDyKm5kgEdgC7vIR+bRPmfQm4UUSyxFX8NnZWCi4RnARchbuTqLlxvARMEpEjRCQOuKPW+ETcmeVuERkB/Dhg3CZc8cMh9Sz7LaCfiPxYXIX9RNw+OC3I2H5ARGIDP7g6pDLgVyISLSKjgTOBF0QkRtxzDUmquhe3faq95ZwhIn1ERIDtuP8f1XWu1LlNRFJEJBu4CVf2Di4x/peIDPCWmyQi5x/s76vHOBE5Tlzl/x+AGapaSAPb1/u/9DbwkBd3tIgc38Jx+aJDJAIvox8DvCwi83BFHz1qTXYhMNU7U28JPwWeUNU1qrq+5gM8AFwsIlGq+jWuAjUTtwMBoKqzcAedB3AVUctxZZB1UtXFwN9xZZUbcBWNXwRM8hjuYL8AmIvbmStx/xHBJaYYXIXyNmAqP9w+TRJETM1xK+7guBP325py299jwLvAfGAOQRQFqmoB8CUuub3R3DhU9W1cuf9HuL9t7ZOPnwN3ichO4Pd4Z9TevGXAn4AvvKKRkbWWvQV3V9MvgS3Ar4AzAoonm6onrmw78JONO/Cfhis2ewhXv7XUm+dSoMArLrsGd6ID0Bd3o8Iu3H7xkKpOb2DdrwOzcXVLbwKPe7/xVeBuXOLZgbvSPK2+hTSg5s6rms8vAsY9h0vQW3E3BVzirbux7Xsprl5wKa4O4OaDiKvNEa9So93xyt2nqepAEekCfKeq9R7cRGQucF2ty8sOSUROAx5R1dqXuMa0CSKiuCK35T6s+0lcJfnvWnvdbVWHuCJQ1R3AqprLR6+4ZUjNeO+yPgV3ltLhiEhncfd2R4lIT9yZzquNzWeMMdBOE4GIPI87qB8mIkUicgXu8vQKEZkPLMLdi1zjQtztYu3z8qdxAvwPrthnLu5+99/7GpExpt1ot0VDxhhjWka7vCIwxhjTctpd42Hp6emam5vrdxjGGNOuzJ49e7OqZtQ1rt0lgtzcXGbNmuV3GMYY066ISL1P2VvRkDHGhDlLBMYYE+YsERhjTJhrd3UExpiOZe/evRQVFbF7926/Q+kQYmNjycrKIjo6Ouh5LBEYY3xVVFREYmIiubm5SN0vrTNBUlW2bNlCUVERvXv3Dno+Kxoyxvhq9+7dpKWlWRJoASJCWlpak6+uLBEYY3xnSaDlHMy2DJtEsHzjLu76z2IqKhtqHt0YY8JP2CSCwq1lTPliFR8t3eh3KMaYNqSkpISHHnqo8QlrGTduHCUlJSGIqPWFTSIY1TedromdmDo7mFedGmPCRX2JoLKyso6p93vrrbdITu4Yb60Mm0QQFRnBOcOymP7dJjbutNvUjDHO7bffzooVKxg6dChHHnkko0aNYvz48RxxxBEAnHXWWQwfPpwBAwYwefLkffPl5uayefNmCgoK6N+/P1dddRUDBgzglFNOoby83K+fc1DC6vbR84Zn8cgnK3ht7lquPv5Qv8MxxtTyP/9ZxOLiHS26zCMyu3DHmQPqHf/nP/+ZhQsXMm/ePD7++GNOP/10Fi5cuO/2yylTppCamkp5eTlHHnkk5557LmlpaQcsY9myZTz//PM89thjXHDBBbzyyitccsklLfo7QilsrggA+nRNIC8nmamzi7D3MBhj6jJixIgD7sG/7777GDJkCCNHjqSwsJBly5b9YJ7evXszdOhQAIYPH05BQUFrhdsiwuqKAOD84dn85tVvWVC0nSHZHaN8z5iOoqEz99YSHx+/r/vjjz/mgw8+4KuvviIuLo7Ro0fXeY9+p06d9nVHRka2u6KhsLoiADhjSA9ioyN42SqNjTFAYmIiO3furHPc9u3bSUlJIS4ujqVLlzJjxoxWjq51hF0i6BIbzdgB3XljXjG791b5HY4xxmdpaWkce+yxDBw4kNtuu+2AcWPHjqWyspL+/ftz++23M3LkSJ+iDK12987i/Px8be6LaT5ftplLHv+a+y7KY/yQzBaKzBhzMJYsWUL//v39DqNDqWubishsVc2va/qwuyIAOObQNHomd+blWVY8ZIwxYZkIIiKEc4f15PPlm1m3vX1V6hhjTEsLy0QAcN7wbFTh33PW+h2KMcb4KmSJQESmiMhGEVlYz/iLRWSBiHwrIl+KyJBQxVKXnLQ4juqdysuzCu2ZAmNMWAvlFcGTwNgGxq8CTlDVQcAfgMkNTBsS5+dnU7CljFmrt7X2qo0xps0IWSJQ1U+BrQ2M/1JVa47AM4CsUMVSn9MGdicuJtIqjY0xYa2t1BFcAbzd2iuN7xTF6YN68OaCdZRVNNzSoDHGACQkJABQXFzMeeedV+c0o0ePprHb3O+9917Kysr29fvZrLXviUBETsQlgl83MM3VIjJLRGZt2rSpRdd/fn42pRVVvP3t+hZdrjGmY8vMzGTq1KkHPX/tROBns9a+JgIRGQz8E5igqlvqm05VJ6tqvqrmZ2RktGgMR+amkJsWZ01OGBOmbr/9dh588MF9/XfeeSd//OMfGTNmDMOGDWPQoEG8/vrrP5ivoKCAgQMHAlBeXs6FF15I//79Ofvssw9oa+jaa68lPz+fAQMGcMcddwCuIbvi4mJOPPFETjzxRGB/s9YA99xzDwMHDmTgwIHce++9+9YXquaufWt0TkRygH8Dl6rq9z7GwXnDs/jbe9+zZksZOWlxfoVijHn7dlj/bcsus/sgOO3P9Y6eOHEiN998M9dddx0AL730Eu+++y433ngjXbp0YfPmzYwcOZLx48fX+z7ghx9+mLi4OJYsWcKCBQsYNmzYvnF/+tOfSE1NpaqqijFjxrBgwQJuvPFG7rnnHqZPn056evoBy5o9ezZPPPEEX3/9NarKUUcdxQknnEBKSkrImrsO5e2jzwNfAYeJSJGIXCEi14jINd4kvwfSgIdEZJ6INK/diGY4Z1gWIjB1TpFfIRhjfJKXl8fGjRspLi5m/vz5pKSk0L17d37zm98wePBgTjrpJNauXcuGDRvqXcann36674A8ePBgBg8evG/cSy+9xLBhw8jLy2PRokUsXry4wXg+//xzzj77bOLj40lISOCcc87hs88+A0LX3HXIrghU9aJGxl8JXBmq9TdFZnJnjuuTziuzi7h5TF8iIurO+saYEGvgzD2Uzj//fKZOncr69euZOHEizz77LJs2bWL27NlER0eTm5tbZ/PTjVm1ahV/+9vfmDlzJikpKUyaNOmgllMjVM1d+15Z3FacNzyLtSXlzFhZb1WFMaaDmjhxIi+88AJTp07l/PPPZ/v27XTt2pXo6GimT5/O6tWrG5z/+OOP57nnngNg4cKFLFiwAIAdO3YQHx9PUlISGzZs4O23998cWV/z16NGjeK1116jrKyM0tJSXn31VUaNGtWCv/aHwu7FNPU5dUB3EmOjeHl2Ecf0SW98BmNMhzFgwAB27txJz5496dGjBxdffDFnnnkmgwYNIj8/n8MPP7zB+a+99louu+wy+vfvT//+/Rk+fDgAQ4YMIS8vj8MPP5zs7GyOPfbYffNcffXVjB07lszMTKZPn75v+LBhw5g0aRIjRowA4MorryQvLy+kbz0Ly2ao6/PbV7/llTlFfPPbk+gSGx2SdRhjDmTNULc8a4a6Gc4bnsXuvdW8uWCd36EYY0yrsUQQYGh2Mn26JjB1tt09ZIwJH5YIAogI5w/PYvbqbazYtMvvcIwJG+2tiLotO5htaYmglrOH9SQyQuyqwJhWEhsby5YtWywZtABVZcuWLcTGxjZpPrtrqJauibGM7pfBv+cUcesphxFpzxQYE1JZWVkUFRXR0u2IhavY2FiysprWmLMlgjqcNzyLD5du5NNlmzjxsK5+h2NMhxYdHU3v3r39DiOsWdFQHcb070ZKXLQVDxljwoIlgjrEREUwYWhP3l+0gZKyCr/DMcaYkLJEUI/z87OoqKrmjfnFfodijDEhZYmgHgMykziiRxdenmXFQ8aYjs0SQQPOG57Ft2u3s3T9Dr9DMcaYkLFE0ICz8noSHSlMtasCY0wHZomgAanxMYw5vBuvzVvL3qpqv8MxxpiQsETQiPPzs9i8q4LpSzf6HYoxxoSEJYJGnNAvg4zETrxszxQYYzooSwSNiIqM4Jy8nkxfupHNu/b4HY4xxrQ4SwRBOG94FpXVymtz1/odijHGtDhraygIfbslMiQ7mZdnFTH6sAxUQYFqVdetrhv2dyuuJcBqBXDfnaMjGZDZBRFryM4Y03ZYIgjSBflZ/PbVhZx0z6fNWs6gnkncfFJffnR4V0sIxpg2wRJBkC7IzyYtPoa9VYoICEKEgDuW13QLAkREuPHePyJEEIHCreU8/MlyrnhqFkOykrjl5H6c0C/DEoIxxlf28vpWtreqmldmF3H/R8tZW1LOsJxkbjm5H8f1SbeEYIwJmYZeXm+JwCcVldW8PLuQBz5azrrtuzkyN4VbTu7HMYem+x2aMaYDskTQhu2prOLFmYU8OH05G3bs4ajeqfzi5H4cdUia36EZYzqQhhJByG4fFZEpIrJRRBbWM15E5D4RWS4iC0RkWKhiacs6RUXyk6Nz+eS2E7njzCNYubmUiZNncPE/ZzCrYKvf4RljwkAonyN4EhjbwPjTgL7e52rg4RDG0ubFRkdy2bG9+exXJ/K70/vz3fqdnPfIV1z6+NfMWbPN7/CMMR1YyBKBqn4KNHRKOwF4Wp0ZQLKI9AhVPO1FbHQkV446hE9/dSL/ddrhLCrewTkPfcmkJ75hfmGJ3+EZYzogP28f7QkUBvQXecPW1Z5QRK7GXTWQk5PTKsH5LS4mip+dcCiXjOzFU18VMPnTlUx48AsO757IGYN7cPrgTHqnx/sdpjGmAwhpZbGI5ALTVHVgHeOmAX9W1c+9/g+BX6tqgzXBHa2yOFg7d+/lldlFTFuwjlmrXVHRgMwunDE4kzMG9yA7Nc7nCI0xbVlDlcV+XhGsBbID+rO8YaYOibHRTDq2N5OO7U1xSTlvfbuOaQvWcfc7S7n7naUMyUrijMGZnD64B5nJnf0O1xjTjvh5RXA6cD0wDjgKuE9VRzS2zHC9IqhP4dayfUnh27XbARiWk7wvKXTrEutzhMaYtsCX5whE5HlgNJAObADuAKIBVPURcY/RPoC7s6gMuKyxYiGwRNCQgs2lvOklhSXrdiACR/ZK5YwhPThtYA8yEjv5HaIxxif2QFkYWrFpF28uWMe0BcV8v2EXEQIjD0njwhE5jB3QnZgoa4HcmHBiiSDMfb9hJ9PmF/PavGLWbC0jI7ETF43I4ccjcuieZEVHxoQDSwQGgOpq5ZNlm3jmq9VM/24jESKcOqAbPzk6l6N6p1qjd8Z0YG31riHTyiIihBMP68qJh3VlzZYy/vX1al6cWchb366nX7cELj06l7PzepLQyXYLY8KJXRGEufKKKv4zv5inZxSwcO0OEjpFce6wnlx6dC59uib4HZ4xpoVY0ZBplKoyt7CEp78s4K1v11NRVc2xfdK4dGQuJ/XvSlSkVS4b055ZIjBNsnnXHl6cWcizM1ZTvH03mUmxXDyyFxOPzCY9oRPqvau52nsns7K//4B3Nlfv765WJVKE5Lhoq4swxgeWCMxBqayq5oMlG3lmRgFfLN/SIsvMSOzE0OxkhmYnk5eTzOCsZKuTMKYVWGWxOShRkRGMHdidsQO7s3zjTt7+dj17q6rdu5nFvYt537uaA/vZ3x84fE9lNYuKdzCvsIT3F28A3Duf+3VN3JcYhuYk07drIpERdtVgTGuxKwLji5KyCuYVljB3TQnzCt1ne/leAOJjIhmUlUReTopLENnJdLWmMoxpFrsiMG1OclwMow/ryujDugKusrpgSxnzCrftSw6PfbqSymp3opKZFEterxR+MrKXvcbTmBZmicC0CSJC7/R4eqfHc3ZeFgC791axqHgHc9dsY15hCV+t2MKbC9Yxqm86vzi5H3k5KT5HbUzHYInAtFmx0ZEM75XC8F7ugL97bxX/mrGahz5ewdkPfcmYw7tyy8n9GNgzyedIjWnfGq0jEJFDgSJV3SMio4HBuFdM+vLeRKsjMKV7KnnyywIe/WQFO3ZXctrA7txycj/6dUv0OzRj2qxm3T4qIvOAfCAXeAt4HRigquNaOM6gWCIwNbaX7+Xxz1cx5fNVlFZUMn5IJjeN6cshGfZEtDG1NTcRzFHVYSJyG7BbVe8XkbmqmheKYBtjicDUtq20gkc/XclTXxZQUVXNOXk9uXFM32a/vrOsopIFRduZu6aEuWu2sWzjLvJykpkwtCfHHppmT1ubdqW5ieBr4F7gt8CZqrpKRBbW9dax1mCJwNRn0849PPzxCv719WpUlQvys7n+R33okdT4qzurq5VVW0r3HfTnrinhuw07qfLuWspNi+PQjAS+KdjKzt2VpCfEcMbgTMYPzSQvO9meljZtXnMTwRHANcBXqvq8iPQGLlDVu1s+1MZZIjCNWbe9nAenL+fFmYWICBcflcPPR/c54A1t28v2Mrdw2wHPMtQ8x5DYKYqhOe75hbycFIZkJ5MaHwO4CuuPv9vE6/PW8uHSjVRUVpOTGseEoZlMGJpJn65WT2HaphZrYkJEUoBsVV3QUsE1lSUCE6zCrWXc/9EyXpmzlpjICC7Iz2LXnirmFm5j5aZSwD3ZfFi3RPJyksnLTiEvJ5lDMxKICOLJ5h279/LuwvW8Mb+YL5ZvplrhiB5dmDDUXSkEcyViTGtp7hXBx8B43K2ms4GNwBeq+osWjjMolghMU63aXMo/Pvie1+cXkxoX4w76OSnkZSczOLtl2jrauHM30+av4/X5xcwvLEEERuSmMmFoT8YN6k5yXEwL/BJjDl5zE8FcVc0TkStxVwN3iMgCVR0cimAbY4nAHKzyiipioyNCXp5fsLmUN+YX89q8tazcVEp0pHBCvwzOHJJJ18RYIiP2t9FU0+3aYwroj/D6a9prihCiI4T0hE5BXa0YU1tzm5iIEpEewAW4CmNj2qXOMZGtsp7c9HhuHNOXG37Uh0XFO3h93lremF/MB0s2NnvZXWKjGJKdzJCsZO87ydphMs0WTCK4C3gXVxw0U0QOAZaFNixj2j8RYWDPJAb2TOL20/qzqHg7u/ZUogpV1eq9z0GproYqVdR7v0N94/ZUVrNk3U7mF5bw8Ccr9t3R1CMpdn9iyE5iUM8kEmOjff71pj2x1keNaYfKK6pYvG478wq3M7+whPlFJazeUga4CvBDMxIYkpXM0OwkhmQnc3j3LsRE2XMP4axZRUMikgXcDxzrDfoMuElVi1ouRGNMU3SOiWR4r1SG90rdN2xbaQUL1nqJobCET77fyCtz3H/TmMgI+md2YXS/DMYN6kG/bgn27IPZJ5jK4veB54BnvEGXABer6skhjq1OdkVgTHBUlbUl5cwv3M6CohJmrd7GnDXbUIVD0uMZO7A74wb1YEBmF0sKYaDZbQ2p6tDGhrUWSwTGHLyNO3fz7qINvLNwHTNWbqWqWslK6cy4QT0YO7A7Q7OS7a6kDqq5ieBD4AngeW/QRcBlqjomiBWPBf4BRAL/VNU/1xqfAzwFJHvT3K6qbzW0TF8SgSoUfgOFM+CICZCS27rrNyYEtpZW8P7i9by9cD1fLN/M3iqlR1Ispw7ozmkDu5Ofm2qvDO1AmpsIeuHqCI4GFPgSuEFVCxuZLxL4HjgZKAJmAhep6uKAaSYDc1X1Ya8pi7dUNbeh5bZqIijbCgtehNlPwaYlbphEwqDzYdQvIOOw1onDmBDbXr6XD5ds4K1v1/Ppsk1UVFaTntCJUwd047SBPRh5SKo1stfONauyWFVX454sDlzg34BbG5l1BLBcVVd687wATAAWB0yjQBevOwkobiyekFOF1V/C7Cdh8etQtQcyh8GZ90GvY2H2EzBriksQ/c+AUb+ETF8aYjWmxSR1juacYVmcMyyLXXsqmb50I28vXMe/56zl2a/XkBIXzclHdGN4rxT6dUukb7fEFnki27QNB3X7qIisUdWcRqY5Dxirqld6/ZcCR6nq9QHT9ADeA1KAeOAkVZ1dx7KuBq4GyMnJGb569eomx9yo0s0w/3l39r9lGXTqAoMnwvCfQvdBtabdAl8/At88Cru3w6FjXELIPbbuZRvTTpVXVPHJ9xt5e+F6PlqykZ17KveNy0rpzGHdEunXPdF9d0vk0K7xdIpqnQf3WkN1tbKropLtZXvZubuStIQYuiZ2apeV6y3W6FzAAgtVNbuRaYJJBL/wYvi7iBwNPA4MVNXq+pbbokVD1dVQ8Kk7+C/5D1TvheyjYPgkOOIsiGmkPfvdO2DW4/DVg1C6CXKOdgmhz0nuZm5jOpDqaqVwWxnfrd/J9xt28pgegaQAABwCSURBVN2GXXy/ficrNu2i0nu4LTJCyE2L47DuLjHUJIrctHhf6xuqq5Xi7eVsLa1ge/neAz47yiu971rDd7th1bUOkXExkfRKiyc3LY7c9Hh6p8WTm+76M9pwkjioRCAiqXWOAAHmq2pWIys9GrhTVU/1+v8LQFX/X8A0i3DJotDrXwmMVNV6n8VvkUSwcwPMexbmPA3bVkFsMgy5yJ39d+3f9OXtLYc5z8AX/4AdRe4KYtQvof94iOg4Z0fG1KWispqCLaX7E4T3vXprGTWHl5ioCPpkJJCbHkd2ShxZqXFkp3QmOzWOnsmdiY1umf8nO3bvZeWmUlZu2uW+N7vvVZtL2VNZ9/llTGQEXTpH06VzFEmdo/d9usRGH9CfEBvF5l17KNhcRsGWUgo2l7Jma9m+JAgQ7yWJ3unx9KpJFOnx5KbFk54Q42uSONhEsApXhl9X5KqqhzSy0ihcZfEYYC2usvjHqrooYJq3gRdV9UkR6Q98CPTUBi5TDjoRVFfDyo9c2f93b0N1JfQ6zh38+4+H6BZor6WyAr59CT7/P9iyHNL6wnG3wOALINIe+TfhpbyiiuUbd/Hdhv0JYs3WMtZuK6ei6sCDcrcunchOiSPbSxAuUcSRndqZHkmdD7iaqKyqpmhb+b6D/IqaA//mUjbt3LNvusgIISc1jkPS4zkkI57e6Ql0TexEUtyBB/nmNERYWVVNccluVnmJoWDfdxmFdSSJpM7RREdFEBMZQUxUBNHedydvWE1/zbhO+7qFmMhIjsxN4Zg+6QcVa4sXDTVhxeNwbzeLBKao6p9E5C5glqq+4d0p9BiQgEs6v1LV9xpa5kEngjlPwxs3QFwaDP0xDPsppPdt+nKCUV3lKpo/uwc2fAtJ2XDsTZB3CURbG/UmvFVXKxt27qZwazmFW8so3FbmureVUbS1jHU7dhN4WIqKEDKTO5OZHMuWXRWs3lJ2QCJJiYvmkIwE74CfwCEZ8RyaEU9OaryvzWpUVlWztqScVZtLWb3FXUXs2l1JRVU1e6uqqaisZk+l+95bVU2FN8z1qzeuypteqapWfj76UH419vCDise3RBAKB50IyktgxYdw+BkQ1anx6VuCKix7Hz77GxR+DfFd4aQ7YMiPIcJuxTOmLhWV1RSXeIlhW02yKKe4pJzU+Bh3oE93B/xDMhL2vT2uo6tpjDD6IG/jtUTgN1VY/QV8eJdLCD3zYdxfoecwvyMzxoSJhhKBnZa2BhHIPQ4ufxfOfhRK1sBjP4L/3ORuRTXGGB8FlQhE5DgRuczrzvBeYG+aSgSGXAg3zIKjr3N3Gt0/DGb+09UrGGOMDxpNBCJyB/Br4L+8QdHAv0IZVIcXmwSn/gmu/QJ6DIY3fwmTT4A1X/sdmTEmDAVzRXA2romJUgBVLQYSQxlU2OjaH37yBpz/pGvXaMop8Oo17jkHY4xpJcEkggrvvn4FEJH40IYUZkRgwNlw/Uz3ENrCV+D+4e5p5aq9fkdnjAkDwSSCl0TkUSBZRK4CPsDd+29aUkw8jPk9/HwG5IyEd38DjxwHKz/xOzJjTAfXaCJQ1b8BU4FXgMOA36vq/aEOLGylHQoXvwwXveCarnh6PLw8Cbbbm0GNMaFhzxG0ZXvL4cv74bO/g0S4oqNjbmi9B+KMMR1Gs54jEJGdIrKj1qdQRF4VkQbbGzLNFN0ZTvgVXPcN9BkDH/0BHj4GVkz3OzJjTAcSTB3BvcBtQE8gC/dCmueAF4ApoQvN7JPSCyb+Cy55BbQanjkLXr4MdqzzOzJjTAcQTCIYr6qPqupOVd2hqpOBU1X1RdwLZUxr6XMSXPsVjP4NLH0THjjSu7uosvF5jTGmHsEkgjIRuUBEIrzPBcBub1z7qmDoCKJjYfSv4bqAu4smnwBrZvgdmTGmnQomEVwMXApsBDZ43ZeISGfg+oZmNCGUeoi7u2jiv1zLqlNOhdeuc6/cNMaYJrC7hjqCilL45C/w1QMQk+Cauh42yZq6Nsbs06xmqEUkFrgCGADse42Xql7ekkEGyxJBAzYude0Wrf4ceg6H0/8OmXl+R2WMaQOa2wz1M0B34FTgE9ydQztbLjzTYroeDpOmwTmPQUmha+r6zVtd0ZExxtQjmETQR1X/GyhV1aeA04GjQhuWOWgi7h3J18+EI6+CWY/DA/kw/wVoZ8WAxpjWEUwiqGn5rEREBgJJQNfQhWRaROdkGPcXuGo6JOfAqz+DJ0+HrSv9jswY08YEkwgmi0gK8DvgDWAxcHdIozItJ3MoXPEBnPkPWL8QHhllVwfGmAM0mAhEJALYoarbVPVTVT1EVbuq6qOtFJ9pCRERMHySexFO98Hu6uDfV8Hu7X5HZoxpAxpMBKpaDfyqlWIxoZac7SqTT/wdLPy3a+ba3opmTNgLpmjoAxG5VUSyRSS15hPyyExoRETCCbfB5e8AAk+cBh/fbc1UGBPGgnmOYFUdg1VVfWl51J4jaEG7d8Bbt8KCFyHnaDhnsqtYNsZ0OM16jkBVe9fxseanO4LYLu7gf/ZkV5H88HHuVZnGmLASzPsI4kTkdyIy2evvKyJnBLNwERkrIt+JyHIRub2eaS4QkcUiskhEnmta+KZFDJkI13wGGf1g6uXw2s9hjz0zaEy4CKaO4AmgAjjG618L/LGxmUQkEngQOA04ArhIRI6oNU1f4L+AY1V1AHBz8KGbFpXaGy57G47/Fcx/Hh49HtbO9jsqY0wrCCYRHKqqf8F7sExVywAJYr4RwHJVXamqFbgX2UyoNc1VwIOqus1b9sagIzctLzIafvRb+Ok0qKyAx0+Bz/8Pqqv9jswYE0LBJIIKr8lpBRCRQ4E9QczXEygM6C/yhgXqB/QTkS9EZIaIjK1rQSJytYjMEpFZmzZtCmLVpllyj4VrP4fDz4AP7oRnJsCOYr+jMsaESDCJ4E7gHSBbRJ4FPqTlni2IAvoCo4GLgMdEJLn2RKo6WVXzVTU/IyOjhVZtGtQ5Bc5/EiY8CEWz3buSl0zzOypjTAgEc9fQe8A5wCTgeSBfVT8OYtlrgeyA/ixvWKAi4A1V3auqq4DvcYnBtAUikHeJq0hOyYUXL4aP/mjNUxjTwQRz19B/gFOAj1V1mqoG+wqsmUBfEektIjHAhbi2igK9hrsaQETScUVF1ipaW5N2KFz+HuRdCp/+FV671tUhGGM6hGCKhv4GjAIWi8hUETnPe1lNg1S1Evcqy3eBJcBLqrpIRO4SkfHeZO8CW0RkMTAduE1VtxzULzGhFRUD4++H0b9xdxU9d4F7IM0Y0+4F/apK73bQH+Hu9Bmrql1CGVh97MniNmDus/CfGyGjv3tvcpcefkdkjGlEc99QhnfX0LnANcCRwFMtF55pd/Iuhh+/CNtWwT9Pgo1L/I7IGNMMwdQRvIQr2vkR8ADuuYIbQh2YaeP6nASXvQXVe2HKqVDwud8RGWMOUjBXBI/jDv7XqOp04BgReTDEcZn2oMcQuPIDSOgOz5xt7RQZ004Fc/vou8BgEfmLiBQAfwCWhjow004k57gmrXvmu3aKvrzfbi81pp2Jqm+EiPTDPeR1EbAZeBFXuXxiK8Vm2ou4VLj0Vffms/d+B9uL4NT/de8+MMa0efUmAtxZ/2fAGaq6HEBEbmmVqEz7Ex0L5z0B72fBVw/AjrVwzmMQ3dnvyIwxjWioaOgcYB0wXUQeE5ExBNfYnAlXERFw6p/g1P/nmqN4egKU2mMhxrR19SYCVX1NVS8EDsc97HUz0FVEHhaRU1orQNMOHf1z105R8TyYcgpsresld8aYtiKYyuJSVX1OVc/EtRc0F/h1yCMz7duAs+Anr0PpZnj8ZFg7x++IjDH1COqBshqqus1rCXRMqAIyHUivo+GK9yGqMzx5Onz/nt8RGWPq0KREYEyTZfRzzxqk94XnL4Tv3vE7ImNMLZYITOgldoNJb0L3QfDKlbDRHkMxpi2xRGBaR6dEuPA5dzvpCxdB+Ta/IzLGeCwRmNaT1BMm/ss9cPbyZVBV6XdExhgsEZjWlnMUnH4PrJwO7//e72iMMTT8ZLExoTHsUtiwEGY8CN0HwtAf+x2RMWHNrgiMP075E/Q+Hv5zExTO9DsaY8KaJQLjj8goOP8p6JIJL14MO4r9jsiYsGWJwPgnLhUufB4qSuGFi2Fvud8RGROWLBEYf3U7As6ZDMVzXDGRvcvAmFZnicD47/DT4cTfwoIX3YttjDGtyhKBaRuOvw2OmAAf3AHLPvA7GmPCiiUC0zaIwFkPQ9cB7pWXm5f5HZExYcMSgWk7YuLhoufcHUXPXwS7t/sdkTFhwRKBaVuSc+CCZ2DbKph6BVRX+R2RMR2eJQLT9uQeC6f9BZa/Dx/+j9/RGNPhhTQRiMhYEflORJaLyO0NTHeuiKiI5IcyHtOOHHkF5F8OX/wDFrzkdzTGdGghSwQiEgk8CJwGHAFcJCJH1DFdInAT8HWoYjHt1Ni7odex8MYN9qpLY0IolFcEI4DlqrpSVSuAF4AJdUz3B+BuYHcIYzHtUVQMXPA0xHd1Tx7vXO93RMZ0SKFMBD2BwoD+Im/YPiIyDMhW1TcbWpCIXC0is0Rk1qZNm1o+UtN2xae7O4l2l8CLl7rmKIwxLcq3ymIRiQDuAX7Z2LSqOllV81U1PyMjI/TBmbal+yD3jEHRN/DAkbDoVWuKwpgWFMpEsBbIDujP8obVSAQGAh+LSAEwEnjDKoxNnQacBZe9A51T4eVJ8PR4e/exMS0klIlgJtBXRHqLSAxwIfBGzUhV3a6q6aqaq6q5wAxgvKrOCmFMpj3rdTT87BMY9zdYtwAeORbe+Y09eGZMM4UsEahqJXA98C6wBHhJVReJyF0iMj5U6zUdXEQkjLgKbpgDeZfAjIfg/nyY9zxUV/sdnTHtkmg7K2vNz8/XWbPsosF41s6Bt26DtbMg+ygY91foMcTvqIxpc0RktqrWWfRuTxab9q3nMLjifZjwIGxZAY+eANNugbKtfkdmTLthicC0fxERrpjohtlw1DUw+ym4fxjMmmJtFRkTBEsEpuPonAyn/Rmu+cw1Zz3tFnjsRCj8xu/IjGnTLBGYjqfbAJg0Dc59HHZthMdPhtd+7rqNMT9gicB0TCIw6Dy4fhYcd4truO7+4fDZ3+3pZGNqsURgOrZOCXDSnfDzGa4Buw/vgn8MhW8eg8oKv6Mzpk2wRGDCQ3of+PELcPl7kNYH3roVHsiH+S9ahbIJe5YITHjJOQouewsungqxXeDVq+GRUfDd29Z+kQlblghM+BGBvifD1Z+6CuXKcnj+QphyKhR84Xd0xrQ6SwQmfEVEuArl676BM+6FkjXw5Dj413muLSNjwoQlAmMioyH/MrhxLpx8FxTNhEdHwdTL3dPKxnRwlgiMqRHdGY69CW6aD6NudfUGDxwJ/7kJdhT7HZ0xIWOJwJjaOifDmP+GG+fBkVfA3Gfhvjx477+hvMTv6IxpcZYIjKlPYjfXmukNs+CIs+DL+702jJ6wW05Nh2KJwJjGpOTCOY/Czz6F9MNg2s0w+QRY/aXfkRnTIiwRGBOsHoPdMwjnTYGybfDEafDyZbC9yO/IjGkWSwTGNIUIDDwXrp8JJ9wO373l3pD28d2wt9zv6Iw5KJYIjDkYMXFw4n+5hNDvVPj4f+GBEbDoNXtC2bQ7lgiMaY7kHLjgKZj0JsQmwcs/hafOhPUL/Y7MmKBZIjCmJeQeBz/7BE6/BzYscg+kTfuFvTLTtAuWCIxpKRGR7rmDG2bDkVfB7Cfd8wdfT4aqSr+jM6ZelgiMaWlxqTDuL3DtF5A5FN6+zV0hrPzY6g9MmxTldwDGdFhd+8Olr8HSN+Hd38DTE6BTF8g4zPv0h4zDXXdSlrsjyRgfWCIwJpREoP8Z0Ock+PYl16rppqXw/bsw91/7p4tJgPR++xNDzXdyL9dKqjEhZInAmNYQHQvDfnLgsNItsPk7lxg2ed8rp8P85/ZPE9UZMgISRM98yMqHmPjWjd90aCFNBCIyFvgHEAn8U1X/XGv8L4ArgUpgE3C5qq4OZUzGtBnxaRB/DPQ65sDh5dtg0/cHJoiCL2DBi268REKPIW6+nJGQPRISMlo/ftNhiIao8kpEIoHvgZOBImAmcJGqLg6Y5kTga1UtE5FrgdGqOrGh5ebn5+usWbNCErMxbVp5iXtXwpqvYPVXsHY2VO1x49L6QM7R3mckpB5idQ7mACIyW1Xz6xoXyiuCEcByVV3pBfECMAHYlwhUdXrA9DOAS0IYjzHtW+dk94rNvie7/so9UDzPJYY1M2DJf2DuM25cQjeXEGoSQ7dBEGklwaZuodwzegKFAf1FwFENTH8F8HYI4zGmY4nqBDlHuQ9AdbWrc6hJDGu+gsWvu3HR8a5uoWt/SOkNqb3dd0ovtxwT1trEKYKIXALkAyfUM/5q4GqAnJycVozMmHYkIsId6Lv2h/zL3bDta/cnhsKvYc4zsLc0YCaBLplecsg9MEmk9obOKX78EtPKQpkI1gLZAf1Z3rADiMhJwG+BE1R1T10LUtXJwGRwdQQtH6oxHVRSTxh0nvuAe6CtdBNsXQXbVu3/3lYA378HpRsPnD822b2PoSY5dMmE+AxX9JTQ1X1iEqw+op0LZSKYCfQVkd64BHAh8OPACUQkD3gUGKuqG3+4CGNMixLZfwDPqaOkds8ulxRqJ4niebD4DdA63swWHffD5BDvfdceFhMX6l9oDkLIEoGqVorI9cC7uNtHp6jqIhG5C5ilqm8AfwUSgJfFnVGsUdXxoYrJGNOITgnQfaD71FZdBWVbYNcG2LXRfUo37u/etQG2rnRFUWVb6l5+TGKtBBH43c3dBpvQzSWWyOjQ/lazT8huHw0Vu33UmHagai+UbnbJoXSTlyjWw65N+xNJqZc8dm+vexlxaT9MGF16QlK2a5IjOcfVYVixVFD8un3UGBOuIqOhSw/3acze3QFXFhsCrjgCvtd85bordx84b3S8lxSyD0wQSVmuP7GH3TYbBNtCxhh/Rce6g3dyI3cEqroip+2FUFLo3hW9vRBK1rju4rk/LJKSSFfBXZMkYrtAVCxEd3bfUbFu/VGd939Hddo/PtrrrxkvEQEtyOr+uAJjrHOc990p0S2zjbFEYIxpH0QgPt19MvPqnqaizEsQXnIITBiFM1xleOVu7/3SPhWLR8e5Yq+4VO878FPXsLSQ15dYIjDGdBwxcV4jff0ank4Vqiq8pLAbKsu97937E8W+7z37x+9LHl69xL76CTmw+4BxXrcq7NnprlrKtnrfW9zdWWVbYU89dSUAnZJckjjyCjjmhqZtkyBYIjDGhB8Rr8ink3vXdFtQWQHlAQli3ydgWEK3kKzaEoExxrQFUTGQ2N19Wpm98cIYY8KcJQJjjAlzlgiMMSbMWSIwxpgwZ4nAGGPCnCUCY4wJc5YIjDEmzFkiMMaYMNfumqEWkU3Aar/jqEc6sNnvIBrQ1uODth+jxdc8Fl/zNCe+XqqaUdeIdpcI2jIRmVVfe99tQVuPD9p+jBZf81h8zROq+KxoyBhjwpwlAmOMCXOWCFrWZL8DaERbjw/afowWX/NYfM0TkvisjsAYY8KcXREYY0yYs0RgjDFhzhJBE4lItohMF5HFIrJIRG6qY5rRIrJdROZ5n9+3cowFIvKtt+5ZdYwXEblPRJaLyAIRGdaKsR0WsF3micgOEbm51jStvv1EZIqIbBSRhQHDUkXkfRFZ5n2n1DPvT71plonIT1sxvr+KyFLvb/iqiCTXM2+D+0MI47tTRNYG/B3H1TPvWBH5ztsfb2/F+F4MiK1ARObVM29It199x5RW3f9U1T5N+AA9gGFedyLwPXBErWlGA9N8jLEASG9g/DjgbdzLVUcCX/sUZySwHvegi6/bDzgeGAYsDBj2F+B2r/t24O465ksFVnrfKV53SivFdwoQ5XXfXVd8wewPIYzvTuDWIPaBFcAhQAwwv/b/p1DFV2v834Hf+7H96jumtOb+Z1cETaSq61R1jte9E1gC9PQ3qiabADytzgwgWUR6+BDHGGCFqvr+pLiqfgpsrTV4AvCU1/0UcFYds54KvK+qW1V1G/A+MLY14lPV91S10uudAWS19HqDVc/2C8YIYLmqrlTVCuAF3HZvUQ3FJyICXAA839LrDUYDx5RW2/8sETSDiOQCecDXdYw+WkTmi8jbIjKgVQMDBd4TkdkicnUd43sChQH9RfiTzC6k/v98fm6/Gt1UdZ3XvR6o683hbWVbXo67yqtLY/tDKF3vFV1Nqadooy1sv1HABlVdVs/4Vtt+tY4prbb/WSI4SCKSALwC3KyqO2qNnoMr7hgC3A+81srhHaeqw4DTgOtE5PhWXn+jRCQGGA+8XMdov7ffD6i7Dm+T91qLyG+BSuDZeibxa394GDgUGAqswxW/tEUX0fDVQKtsv4aOKaHe/ywRHAQRicb9wZ5V1X/XHq+qO1R1l9f9FhAtIumtFZ+qrvW+NwKv4i6/A60FsgP6s7xhrek0YI6qbqg9wu/tF2BDTZGZ972xjml83ZYiMgk4A7jYO1j8QBD7Q0io6gZVrVLVauCxetbr9/aLAs4BXqxvmtbYfvUcU1pt/7NE0EReeeLjwBJVvaeeabp70yEiI3DbeUsrxRcvIok13bgKxYW1JnsD+Il399BIYHvAJWhrqfcszM/tV8sbQM1dGD8FXq9jmneBU0QkxSv6OMUbFnIiMhb4FTBeVcvqmSaY/SFU8QXWO51dz3pnAn1FpLd3lXghbru3lpOApapaVNfI1th+DRxTWm//C1VNeEf9AMfhLtEWAPO8zzjgGuAab5rrgUW4OyBmAMe0YnyHeOud78XwW294YHwCPIi7W+NbIL+Vt2E87sCeFDDM1+2HS0rrgL24ctYrgDTgQ2AZ8AGQ6k2bD/wzYN7LgeXe57JWjG85rny4Zj98xJs2E3irof2hleJ7xtu/FuAOaj1qx+f1j8PdKbOiNePzhj9Zs98FTNuq26+BY0qr7X/WxIQxxoQ5KxoyxpgwZ4nAGGPCnCUCY4wJc5YIjDEmzFkiMMaYMGeJwJhaRKRKDmwhtcVaxBSR3MAWMI1pC6L8DsCYNqhcVYf6HYQxrcWuCIwJktcu/V+8tum/EZE+3vBcEfnIa1ztQxHJ8YZ3E/eegPne5xhvUZEi8pjX9vx7ItLZtx9lDJYIjKlL51pFQxMDxm1X1UHAA8C93rD7gadUdTCu4bf7vOH3AZ+oazxvGO7JVIC+wIOqOgAoAc4N8e8xpkH2ZLExtYjILlVNqGN4AfAjVV3pNRK2XlXTRGQzrvmEvd7wdaqaLiKbgCxV3ROwjFxc+/F9vf5fA9Gq+sfQ/zJj6mZXBMY0jdbT3RR7ArqrsLo64zNLBMY0zcSA76+87i9xrWYCXAx85nV/CFwLICKRIpLUWkEa0xR2JmLMD3WWA19k/o6q1txCmiIiC3Bn9Rd5w24AnhCR24BNwGXe8JuAySJyBe7M/1pcC5jGtClWR2BMkLw6gnxV3ex3LMa0JCsaMsaYMGdXBMYYE+bsisAYY8KcJQJjjAlzlgiMMSbMWSIwxpgwZ4nAGGPC3P8HiK2qmqplwLkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw curve\n",
    "draw_curve(x_epoch, epoch_train_loss, validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "qVSsG_hQSt5x",
    "outputId": "9cf86522-1c87-473d-9a0f-12e51adce44b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_train_curve(x_epoch, epoch_train_loss)\n",
    "draw_val_curve(x_epoch, validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0Nqzc6IZ7Cp",
    "outputId": "649712c7-e22a-452e-c230-1762fe41b96a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "|End of training | test loss 17.2587 | test ppl 31285716.89\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Now, test the model using the testing set\n",
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "mWzFNqjVVonO"
   },
   "outputs": [],
   "source": [
    "# Close the file after writing to log\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WordLevelModel.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
